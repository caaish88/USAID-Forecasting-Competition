{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import catboost\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Read data files and pre-process\n",
    "_DATA_DIR = './input'\n",
    "    \n",
    "logistics_data = 'contraceptive_logistics_data.csv'\n",
    "product = 'product.csv'\n",
    "site = 'service_delivery_site_data.csv'\n",
    "submission_file = 'submission_format.csv'\n",
    "\n",
    "\n",
    "class Dataset():\n",
    "    \"\"\"Monthly stock distribution data.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the data from disk.\"\"\"\n",
    "\n",
    "        self.logistics_data = pd.read_csv(os.path.join(_DATA_DIR, logistics_data))\n",
    "        self.product = pd.read_csv(os.path.join(_DATA_DIR, product))\n",
    "        self.site = pd.read_csv(os.path.join(_DATA_DIR, site))\n",
    "        self.submission = pd.read_csv(os.path.join(_DATA_DIR,submission_file))\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"Preprocess the dataset.\"\"\"\n",
    "\n",
    "        # preprocessing operations go here.\n",
    "        product_df = self._preprocess_product(self.product)\n",
    "        site_df = self._preprocess_site(self.site)\n",
    "        final_df,logistics_df,submission_ID,Zero_demand_df = self._preprocess_logistics(self.logistics_data,product_df,site_df,self.submission)\n",
    "\n",
    "        return final_df,logistics_df,submission_ID,Zero_demand_df\n",
    "\n",
    "    def _preprocess_product(self, product):\n",
    "        product_df = product[['product_code', 'product_type']]\n",
    "\n",
    "        return product_df\n",
    "\n",
    "    def _preprocess_site(self, site):\n",
    "        site_df = site[['site_code', 'site_type']]\n",
    "\n",
    "        return site_df\n",
    "    \n",
    "    def _preprocess_logistics(self, logistics_data,product_df,site_df,submission):\n",
    "        # Add day column\n",
    "        logistics_data['day'] = 1\n",
    "\n",
    "        # Create date column\n",
    "        logistics_data['date'] = pd.to_datetime(logistics_data[['month', 'day', 'year']])\n",
    "        logistics_data['date'] = logistics_data['date'].dt.strftime('%m/%d/%Y')\n",
    "        logistics_data['date'] = pd.to_datetime(logistics_data['date'])\n",
    "\n",
    "        logistics_data['batchID'] = logistics_data['product_code'] + \"_\" + logistics_data['site_code']\n",
    "\n",
    "        # Rename column names\n",
    "        logistics_data = logistics_data.rename(columns={'stock_distributed': 'value'})\n",
    "        \n",
    "        #Merge product and logistics data and bring the product type as feature\n",
    "        logistics_data = pd.merge(logistics_data,product_df,how='left',left_on=['product_code'], right_on = ['product_code'])\n",
    "\n",
    "        #Merge site type from service delivery site file as a feature\n",
    "        logistics_data = pd.merge(logistics_data,site_df,how='left',left_on=['site_code'],right_on=['site_code'])\n",
    "        \n",
    "        # Drop columns\n",
    "        logistics_data = logistics_data.drop(columns=['stock_received', 'stock_adjustment',\n",
    "                                                      'stock_end', 'stock_stockout_days', 'stock_ordered'], axis=1)\n",
    "\n",
    "        # Sort by date in ascending format\n",
    "        logistics_data = logistics_data.sort_values(by='date', ascending=True)\n",
    "\n",
    "        # Add missing time periods and fill missing values with 0\n",
    "        logistics_data = (logistics_data.set_index('date')\n",
    "                          .groupby('batchID')\n",
    "                          .apply(lambda d: d.reindex(pd.date_range(min(logistics_data.date),\n",
    "                                                                   max(logistics_data.date),\n",
    "                                                                   freq='MS')))\n",
    "                          .drop('batchID', axis=1)\n",
    "                          .reset_index('batchID')\n",
    "                          .fillna(0))\n",
    "        \n",
    "        # Set date as a column\n",
    "        logistics_data['date'] = logistics_data.index\n",
    "\n",
    "        a = logistics_data\n",
    "\n",
    "        cols = [\"region\",\"district\",'product_type','site_type',\"site_code\",\"product_code\"]\n",
    "\n",
    "        a[cols] = a[cols].replace({'0':np.nan, 0:np.nan})\n",
    "\n",
    "        a = logistics_data.groupby('batchID')\n",
    "\n",
    "        logistics_data['region'] = a['region'].transform(lambda s: s.loc[s.first_valid_index()])\n",
    "        logistics_data['district'] = a['district'].transform(lambda s: s.loc[s.first_valid_index()])\n",
    "        logistics_data['product_type'] = a['product_type'].transform(lambda s: s.loc[s.first_valid_index()])\n",
    "        logistics_data['site_type'] = a['site_type'].transform(lambda s: s.loc[s.first_valid_index()])\n",
    "        logistics_data['site_code'] = a['site_code'].transform(lambda s: s.loc[s.first_valid_index()])\n",
    "        logistics_data['product_code'] = a['product_code'].transform(lambda s: s.loc[s.first_valid_index()])\n",
    "\n",
    "        # Drop existing month, year and day columns with missing values and extract year from date\n",
    "        logistics_data = logistics_data.drop(columns=['year', 'month', 'day'], axis=1)\n",
    "\n",
    "        logistics_data['year'] = logistics_data.index.year\n",
    "        logistics_data['month'] = logistics_data.index.month\n",
    "        logistics_data['quarter'] = logistics_data.index.quarter\n",
    "\n",
    "        # Convert month, year, and quarter as binary variables - convert to category\n",
    "        logistics_data['month'] = \"Month\" + logistics_data['month'].astype(str)\n",
    "        logistics_data['quarter'] = \"Quarter\" + logistics_data['quarter'].astype(str)\n",
    "        logistics_data['year'] = \"Year\" + logistics_data['year'].astype(str)\n",
    "        \n",
    "        # Create unique product site combination\n",
    "        submission['batchID'] = submission['product_code'] + \"_\" + submission['site_code']\n",
    "\n",
    "        # List of unique batchID\n",
    "        submission_ID = np.unique(submission['batchID'])\n",
    "\n",
    "        #Product Site combinations to be predicted | Historical data available\n",
    "        df_1 = logistics_data[logistics_data['batchID'].isin(submission_ID)]\n",
    "\n",
    "        #Product site combination to be predicted and no historical data available\n",
    "        NoHistorical_ID = np.unique(submission[~submission['batchID'].isin(np.unique(logistics_data['batchID']))]['batchID'])\n",
    "\n",
    "        #Create dataframe for 45 months with 0 as demand for the product site combinations  \n",
    "        Zero_demand_df = pd.DataFrame()\n",
    "\n",
    "        for i in NoHistorical_ID:\n",
    "            date = pd.date_range('1/1/2016', freq='MS', periods=45)\n",
    "            a = pd.DataFrame(i, index=date, columns=['batchID'])\n",
    "            a['date'] = a.index\n",
    "            #a[['product_code','site_code']] = a.batchID.str.split(\"_\",expand=True) \n",
    "            a['value'] = 0\n",
    "            Zero_demand_df = Zero_demand_df.append(a)\n",
    "\n",
    "        # Combine df_1 and df_2 to create historical data for all product site combinations requried to be forecasted\n",
    "        Insample = pd.concat([df_1[['batchID','date','value']],Zero_demand_df])\n",
    "\n",
    "        return Insample,df_1,submission_ID,Zero_demand_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Make sure we can load the dataset\n",
    "    dataset = Dataset()\n",
    "    final_df,logistics_df,submission_ID,Zero_demand_df = dataset.preprocess()\n",
    "    print('Successfully loaded the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train Test data: Forecast Horizon is 3, so last 3 data points from each group kept as test data\n",
    "df_test = final_df.groupby('batchID').tail(3)\n",
    "\n",
    "# Remove the Test data sets from Train data\n",
    "comb = df_test['batchID']+df_test['date'].apply(str)\n",
    "final_df['conc'] = final_df['batchID']+final_df['date'].apply(str)\n",
    "\n",
    "df_train = final_df[~final_df['conc'].isin(comb)]\n",
    "\n",
    "# Drop concatenate column\n",
    "df_train = df_train.drop(columns = ['conc'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Insample error - naive forecasting : 9.98532412130205\n"
     ]
    }
   ],
   "source": [
    "# Compute In sample Naive Forecasting Mean Absolute Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "naive_forecasting_train = df_train\n",
    "\n",
    "#Calculate in sample MAE - naive forecasting\n",
    "naive_forecasting_train['prediction'] = naive_forecasting_train.groupby(['batchID'])['value'].shift(1)\n",
    "naive_forecasting_train = naive_forecasting_train.fillna(0)\n",
    "naive_forecasting_train['MAE'] = abs(naive_forecasting_train.prediction - naive_forecasting_train.value)\n",
    "\n",
    "insample_naive = naive_forecasting_train.groupby('batchID')['MAE'].sum()/(naive_forecasting_train.groupby('batchID')['MAE'].count()-1)\n",
    "\n",
    "#Overall average insample naive forecasting \n",
    "print(\"Average Insample error - naive forecasting :\",insample_naive.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sample Naive MAE - Groupby for comparison\n",
    "insample_naive = pd.DataFrame(naive_forecasting_train.groupby('batchID')['MAE'].mean())\n",
    "insample_naive = insample_naive.reset_index()\n",
    "insample_naive.columns = ['batchID','Insample-MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find TS to be predicted and for which historical data is available\n",
    "df = logistics_df\n",
    "\n",
    "# Count number of zeroes per group\n",
    "count = pd.DataFrame(df.groupby('batchID')['value'].agg(lambda x: x.eq(0).sum()))\n",
    "count['batchID'] = count.index\n",
    "count.columns = ['Zeroes','batchID']\n",
    "count = count.reset_index(drop=True)\n",
    "\n",
    "# Count length of time series per group\n",
    "TSlength =  pd.DataFrame(df.groupby('batchID')['value'].count())\n",
    "TSlength['batchID'] = TSlength.index\n",
    "TSlength = TSlength.reset_index(drop=True)\n",
    "TSlength.columns = ['count','batchID']\n",
    "\n",
    "ZeroCount = count.merge(TSlength,on='batchID',how='inner')\n",
    "ZeroCount['CountZero%'] = (ZeroCount['Zeroes']/ZeroCount['count'])*100\n",
    "\n",
    "# Bin the % of zeroes into a new column\n",
    "bins = [0, 20, 40, 60, 80, 100]\n",
    "labels = [0, 20, 40, 60, 80]\n",
    "categories = ['0','20','40','60','80']\n",
    "ZeroCount['bins'] = pd.cut(ZeroCount['CountZero%'], bins = bins, labels = labels)\n",
    "ZeroCount = ZeroCount.fillna(0)\n",
    "\n",
    "# Filter out TS which have more than 80% of records as zeroes and add to Zero_df\n",
    "Zero_df = pd.concat([df[df['batchID'].isin(ZeroCount[ZeroCount['bins'] == 80].batchID)],Zero_demand_df])\n",
    "df = df[df['batchID'].isin(ZeroCount[ZeroCount['bins'] != 80].batchID)]\n",
    "\n",
    "# Filter out TS with more than 40% and less than 80% of records as zeroes\n",
    "Sixty_df = df[df['batchID'].isin(ZeroCount[ZeroCount['bins'] == 60].batchID)]\n",
    "Forty_df = df[df['batchID'].isin(ZeroCount[ZeroCount['bins'] == 40].batchID)]\n",
    "\n",
    "# Remove TS which have greater than 40% of records as zeroes - on remaining try machine learning algorithms\n",
    "df = df[df['batchID'].isin(ZeroCount[ZeroCount['bins'] != 60].batchID)]\n",
    "df = df[df['batchID'].isin(ZeroCount[ZeroCount['bins'] != 40].batchID)]\n",
    "\n",
    "# Combine Forty and Sixty TS\n",
    "Forty_Sixty_df = pd.concat([Forty_df,Sixty_df])\n",
    "\n",
    "# Remove leading zeroes\n",
    "NonZero_df = df[df.groupby('batchID')['value'].cumsum().gt(0)]\n",
    "Forty_df = Forty_df[Forty_df.groupby('batchID')['value'].cumsum().gt(0)]\n",
    "Sixty_df = Sixty_df[Sixty_df.groupby('batchID')['value'].cumsum().gt(0)]\n",
    "Forty_Sixty_df = Forty_Sixty_df[Forty_Sixty_df.groupby('batchID')['value'].cumsum().gt(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Zero_df into Train and Test and fit naive forecasting.\n",
    "Zero_df = Zero_df.sort_values(by = 'date',ascending=True)\n",
    "Zero_df_Test = Zero_df.groupby('batchID').tail(3)\n",
    "\n",
    "# Remove the Test data sets from Train data\n",
    "combn = Zero_df_Test['batchID']+Zero_df_Test['date'].apply(str)\n",
    "Zero_df['conc'] = Zero_df['batchID']+Zero_df['date'].apply(str)\n",
    "\n",
    "Zero_df_Train = Zero_df[~Zero_df['conc'].isin(combn)]\n",
    "\n",
    "# Drop concatenate column\n",
    "Zero_df_Train = Zero_df_Train.drop(columns = ['conc'],axis = 1)\n",
    "\n",
    "# Drop conc from df\n",
    "Zero_df = Zero_df.drop(columns = ['conc'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Forecasting \n",
    "def naive_forecasting_test(Train,Test):\n",
    "    '''Naive Forecasting \n",
    "    '''    \n",
    "    # initialize empty dataset\n",
    "    columns = ['batchID','Naive_MAE']\n",
    "    datas = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # select unique batch_id\n",
    "    batch_ids = Train['batchID'].unique()\n",
    "\n",
    "    # groupby different batch_id\n",
    "    Train = Train.groupby(['batchID'])\n",
    "    Test = Test.groupby(['batchID'])\n",
    "    \n",
    "    #loop through the batch_ids\n",
    "    for batch_id in batch_ids:\n",
    "        d_train = Train.get_group(batch_id)\n",
    "        d_test = Test.get_group(batch_id)\n",
    "        naive_train = np.asarray(d_train.value)\n",
    "        naive_test = d_test[['batchID','value']]\n",
    "        naive_test['naive'] = naive_train[len(naive_train)-1]\n",
    "        naive_test['Naive_MAE'] = abs(naive_test.value - naive_test.naive)\n",
    "        datas = datas.append(naive_test)\n",
    "    \n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero_df: Naive Forecasting - Average MAE 5.148936170212766\n"
     ]
    }
   ],
   "source": [
    "# Fit Naive Forecasting on Zero_df \n",
    "# Drop date column as it is present in index\n",
    "Zero_df_Naive_Train = Zero_df_Train.set_index('date')\n",
    "Zero_df_Naive_Test = Zero_df_Test.set_index('date') \n",
    "\n",
    "# Fit Naive Forecasting Model on Zero df and Less than 5 data points and calculate out of sample MAE\n",
    "Zero_df_Naive_Train = Zero_df_Naive_Train[['batchID','value']]\n",
    "Zero_df_Naive_Test = Zero_df_Naive_Test[['batchID','value']]\n",
    "\n",
    "# Fit and predict \n",
    "Zero_df_Naive_Forecast = naive_forecasting_test(Train = Zero_df_Naive_Train,Test = Zero_df_Naive_Test)\n",
    "\n",
    "# Calculate in sample naive forecasting for each group\n",
    "Zero_df_Naive_results = Zero_df_Naive_Forecast.groupby('batchID')['Naive_MAE'].mean()\n",
    "Zero_df_Naive_results = Zero_df_Naive_results.reset_index()\n",
    "\n",
    "print(\"Zero_df: Naive Forecasting - Average MAE\",Zero_df_Naive_results['Naive_MAE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify Zero_df TS with greater than MASE of 1 will be fit separately\n",
    "res = pd.merge(Zero_df_Naive_results,insample_naive,on=['batchID'],how='inner')\n",
    "res['MASE'] = res['Naive_MAE']/res['Insample-MAE']\n",
    "res = res.replace([np.inf, -np.inf], 1)\n",
    "res= res.fillna(1)\n",
    "res.sort_values(by='MASE',ascending=False)\n",
    "Zero_TS_NonZero_ID = res[res['MASE']>1].batchID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add TS with > 80% zero demand with MASE > 1 in Non-zero df\n",
    "NonZero_df = pd.concat([logistics_df[logistics_df['batchID'].isin(Zero_TS_NonZero_ID)],NonZero_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero_df Naive Forecasting: Average MAE 0.9338731443994599\n",
      "Zero_df MASE: 0.6293558237074743\n"
     ]
    }
   ],
   "source": [
    "# Fit Naive Forecasting on Zero_df  - on TS with MASE < 1\n",
    "Zero_df_Naive_Train = Zero_df_Naive_Train[~Zero_df_Naive_Train['batchID'].isin(Zero_TS_NonZero_ID)]\n",
    "Zero_df_Naive_Test = Zero_df_Naive_Test[~Zero_df_Naive_Test['batchID'].isin(Zero_TS_NonZero_ID)]\n",
    "\n",
    "# Fit and predict \n",
    "Zero_df_Naive_Forecast = naive_forecasting_test(Train = Zero_df_Naive_Train,Test = Zero_df_Naive_Test)\n",
    "\n",
    "# Calculate in sample naive forecasting for each group\n",
    "Zero_df_Naive_results = Zero_df_Naive_Forecast.groupby('batchID')['Naive_MAE'].mean()\n",
    "Zero_df_Naive_results = Zero_df_Naive_results.reset_index()\n",
    "\n",
    "print(\"Zero_df Naive Forecasting: Average MAE\",Zero_df_Naive_results['Naive_MAE'].mean())\n",
    "\n",
    "# Merge insample naive to calculate MASE\n",
    "res = pd.merge(Zero_df_Naive_results,insample_naive,on=['batchID'],how='inner')\n",
    "res['MASE'] = res['Naive_MAE']/res['Insample-MAE']\n",
    "res = res.replace([np.inf, -np.inf], 0)\n",
    "res = res.fillna(1)\n",
    "print(\"Zero_df MASE:\",res['MASE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create Forecast dataframe from Oct to Dec'19 #######\n",
    "\n",
    "# Non Zero_df\n",
    "NonZero_Forecast_ID = np.unique(NonZero_df['batchID'])\n",
    "\n",
    "#Create Forecast dataframe from Oct'19 to Dec'19   \n",
    "NonZero_Forecast = pd.DataFrame()\n",
    "\n",
    "for i in NonZero_Forecast_ID:\n",
    "    # Date range from Oct'19 to Dec'19\n",
    "    date = pd.date_range('10/1/2019', freq='MS', periods=3)\n",
    "    a = pd.DataFrame(i, index=date, columns=['batchID'])\n",
    "    a['date'] = a.index\n",
    "    a['year'] = a.index.year\n",
    "    a['month'] = a.index.month\n",
    "    a['quarter'] = a.index.quarter \n",
    "\n",
    "    # Convert month, year, and quarter as binary variables - convert to category\n",
    "    a['month'] = \"Month\"+a['month'].astype(str)\n",
    "    a['quarter'] = \"Quarter\"+a['quarter'].astype(str)\n",
    "    a['year'] = \"Year\"+a['year'].astype(str)\n",
    "    a = a.reset_index(drop=True)\n",
    "    a['value'] = 0\n",
    "    a['average_monthly_consumption'] = 0\n",
    "    a['stock_initial'] = 0\n",
    "    NonZero_Forecast = NonZero_Forecast.append(a)\n",
    "\n",
    "# Zero_df\n",
    "Zero_df = Zero_df[~Zero_df['batchID'].isin(Zero_TS_NonZero_ID)] \n",
    "Zero_Forecast_ID = np.unique(Zero_df['batchID'])\n",
    "\n",
    "#Create Forecast dataframe from Oct'19 to Dec'19   \n",
    "Zero_Forecast = pd.DataFrame()\n",
    "\n",
    "for i in Zero_Forecast_ID:\n",
    "    # Date range from Oct'19 to Dec'19\n",
    "    date = pd.date_range('10/1/2019', freq='MS', periods=3)\n",
    "    a = pd.DataFrame(i, index=date, columns=['batchID'])\n",
    "    Zero_Forecast = Zero_Forecast.append(a)\n",
    "\n",
    "# Forty df \n",
    "Forty_Forecast_ID = np.unique(Forty_df['batchID'])\n",
    "\n",
    "#Create Forecast dataframe from Oct'19 to Dec'19   \n",
    "Forty_Forecast = pd.DataFrame()\n",
    "\n",
    "for i in Forty_Forecast_ID:\n",
    "    # Date range from Oct'19 to Dec'19\n",
    "    date = pd.date_range('10/1/2019', freq='MS', periods=3)\n",
    "    a = pd.DataFrame(i, index=date, columns=['batchID'])\n",
    "    a['date'] = a.index\n",
    "    a['year'] = a.index.year\n",
    "    a['month'] = a.index.month\n",
    "    a['quarter'] = a.index.quarter \n",
    "\n",
    "    # Convert month, year, and quarter as binary variables - convert to category\n",
    "    a['month'] = \"Month\"+a['month'].astype(str)\n",
    "    a['quarter'] = \"Quarter\"+a['quarter'].astype(str)\n",
    "    a['year'] = \"Year\"+a['year'].astype(str)\n",
    "\n",
    "    a = a.reset_index(drop=True)\n",
    "    a['value'] = 0\n",
    "    a['average_monthly_consumption'] = 0\n",
    "    a['stock_initial'] = 0\n",
    "    Forty_Forecast = Forty_Forecast.append(a)\n",
    "\n",
    "# Sixty df\n",
    "Sixty_Forecast_ID = np.unique(Sixty_df['batchID'])\n",
    "\n",
    "#Create Forecast dataframe from Oct'19 to Dec'19   \n",
    "Sixty_Forecast = pd.DataFrame()\n",
    "\n",
    "for i in Sixty_Forecast_ID:\n",
    "    # Date range from Oct'19 to Dec'19\n",
    "    date = pd.date_range('10/1/2019', freq='MS', periods=3)\n",
    "    a = pd.DataFrame(i, index=date, columns=['batchID'])\n",
    "    a['date'] = a.index\n",
    "    a['year'] = a.index.year\n",
    "    a['month'] = a.index.month\n",
    "    a['quarter'] = a.index.quarter \n",
    "\n",
    "    # Convert month, year, and quarter as binary variables - convert to category\n",
    "    a['month'] = \"Month\"+a['month'].astype(str)\n",
    "    a['quarter'] = \"Quarter\"+a['quarter'].astype(str)\n",
    "    a['year'] = \"Year\"+a['year'].astype(str)\n",
    "\n",
    "    a = a.reset_index(drop=True)\n",
    "    a['value'] = 0\n",
    "    a['average_monthly_consumption'] = 0\n",
    "    a['stock_initial'] = 0\n",
    "    Sixty_Forecast = Sixty_Forecast.append(a)\n",
    "\n",
    "# Forty Sixty df \n",
    "Forty_Sixty_df_Forecast_ID = np.unique(Forty_Sixty_df['batchID'])\n",
    "\n",
    "#Create Forecast dataframe from Oct'19 to Dec'19   \n",
    "Forty_Sixty_df_Forecast = pd.DataFrame()\n",
    "\n",
    "for i in Forty_Sixty_df_Forecast_ID:\n",
    "    # Date range from Oct'19 to Dec'19\n",
    "    date = pd.date_range('10/1/2019', freq='MS', periods=3)\n",
    "    a = pd.DataFrame(i, index=date, columns=['batchID'])\n",
    "    a['date'] = a.index\n",
    "    a['year'] = a.index.year\n",
    "    a['month'] = a.index.month\n",
    "    a['quarter'] = a.index.quarter \n",
    "\n",
    "    # Convert month, year, and quarter as binary variables - convert to category\n",
    "    a['month'] = \"Month\"+a['month'].astype(str)\n",
    "    a['quarter'] = \"Quarter\"+a['quarter'].astype(str)\n",
    "    a['year'] = \"Year\"+a['year'].astype(str)\n",
    "\n",
    "    a = a.reset_index(drop=True)\n",
    "    a['value'] = 0\n",
    "    a['average_monthly_consumption'] = 0\n",
    "    a['stock_initial'] = 0\n",
    "    Forty_Sixty_df_Forecast = Forty_Sixty_df_Forecast.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Forecasting \n",
    "def naive_forecasting_Forecast(Train,Forecast):\n",
    "    '''Naive Forecasting \n",
    "    '''    \n",
    "    # initialize empty dataset\n",
    "    columns = ['batchID','predicted_value']\n",
    "    datas = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # select unique batch_id\n",
    "    batch_ids = Train['batchID'].unique()\n",
    "\n",
    "    # groupby different batch_id\n",
    "    Train = Train.groupby(['batchID'])\n",
    "    Forecast = Forecast.groupby(['batchID'])\n",
    "    \n",
    "    #loop through the batch_ids\n",
    "    for batch_id in batch_ids:\n",
    "        d_train = Train.get_group(batch_id)\n",
    "        d_Forecast = Forecast.get_group(batch_id)\n",
    "        naive_train = np.asarray(d_train.value)\n",
    "        naive_Forecast = d_Forecast[['batchID']]\n",
    "        naive_Forecast['predicted_value'] = naive_train[len(naive_train)-1]\n",
    "        datas = datas.append(naive_Forecast)\n",
    "    \n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Forecast for 247 TS - 80% of records have zeroes\n",
    "Naive_Train = pd.concat([Zero_df_Naive_Train,Zero_df_Naive_Test])\n",
    "\n",
    "# Forecast \n",
    "Naive_Forecast_Zero_df = naive_forecasting_Forecast(Train = Naive_Train,Forecast = Zero_Forecast)\n",
    "\n",
    "# Add month and year as a column\n",
    "Naive_Forecast_Zero_df['year'] = Naive_Forecast_Zero_df.index.year\n",
    "Naive_Forecast_Zero_df['month'] = Naive_Forecast_Zero_df.index.month\n",
    "Naive_Forecast_Zero_df = Naive_Forecast_Zero_df.reset_index(drop=True)\n",
    "\n",
    "# Split batchID to product and site code and drop batchID\n",
    "ID = Naive_Forecast_Zero_df[\"batchID\"].str.split(\"_\", n = 1, expand = True) \n",
    "\n",
    "Naive_Forecast_Zero_df['site_code']  = ID[1]\n",
    "Naive_Forecast_Zero_df['product_code']  = ID[0]\n",
    "\n",
    "Naive_Forecast_Zero_df = Naive_Forecast_Zero_df.drop(columns=['batchID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add categorical columns from historic data to forecast columns\n",
    "cat_cols = logistics_df.groupby(['batchID','site_type','region','district','site_code','product_code','product_type'])['value'].sum().reset_index()\n",
    "\n",
    "# Merge cat cols to Forecast dataframe\n",
    "NonZero_Forecast = pd.merge(NonZero_Forecast,cat_cols.drop(columns=['value'],axis=1),how='left',on=['batchID'])\n",
    "Sixty_Forecast = pd.merge(Sixty_Forecast,cat_cols.drop(columns=['value'],axis=1),how='left',on=['batchID'])\n",
    "Forty_Forecast = pd.merge(Forty_Forecast,cat_cols.drop(columns=['value'],axis=1),how='left',on=['batchID'])\n",
    "Forty_Sixty_df_Forecast = pd.merge(Forty_Sixty_df_Forecast,cat_cols.drop(columns=['value'],axis=1),how='left',on=['batchID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Product Level Forecasting - Use as a causal to fit machine learning algorithm ########\n",
    "\n",
    "#Product level aggregation\n",
    "products = logistics_df.groupby(['product_code','date']).sum().reset_index()\n",
    "\n",
    "# Split Train Test data: Forecast Horizon is 3, so last 3 data points from each group kept as test data\n",
    "products_Test = products.groupby('product_code').tail(3)\n",
    "\n",
    "# Remove the Test data sets from Train data\n",
    "comb = products_Test['product_code']+products_Test['date'].apply(str)\n",
    "products['conc'] = products['product_code']+products['date'].apply(str)\n",
    "\n",
    "products_Train = products[~products['conc'].isin(comb)]\n",
    "\n",
    "# Drop concatenate column\n",
    "products_Train = products_Train.drop(columns = ['conc'],axis =1)\n",
    "\n",
    "products_Train = products_Train[['product_code','date','value']]\n",
    "products_Test = products_Test[['product_code','date','value']]\n",
    "\n",
    "#Naive Forecasting \n",
    "def naive_forecasting(Train,Test):\n",
    "    '''Naive Forecasting \n",
    "    '''    \n",
    "    # initialize empty dataset\n",
    "    columns = ['product_code','MAE']\n",
    "    datas = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # select unique batch_id\n",
    "    batch_ids = Train['product_code'].unique()\n",
    "\n",
    "    # groupby different batch_id\n",
    "    Train = Train.groupby(['product_code'])\n",
    "    Test = Test.groupby(['product_code'])\n",
    "    \n",
    "    #loop through the batch_ids\n",
    "    for batch_id in batch_ids:\n",
    "        d_train = Train.get_group(batch_id)\n",
    "        d_test = Test.get_group(batch_id)\n",
    "        naive_train = np.asarray(d_train.value)\n",
    "        naive_test = d_test[['product_code','value']]\n",
    "        naive_test['naive'] = naive_train[len(naive_train)-1]\n",
    "        naive_test['MAE'] = abs(naive_test.value - naive_test.naive)\n",
    "        #naive_test['MAE'] = mean_absolute_error(naive_test.value, naive_test.naive)\n",
    "        #d_train = d_train[['batchID','Test','Pred','MAE']]\n",
    "        datas = datas.append(naive_test)\n",
    "    \n",
    "    return datas\n",
    "\n",
    "products_Train = products_Train.set_index('date')\n",
    "products_Test = products_Test.set_index('date')\n",
    "\n",
    "# Fit Naive Forecasting Model \n",
    "products_Train = products_Train[['product_code','value']]\n",
    "products_Test = products_Test[['product_code','value']]\n",
    "\n",
    "product_naive_test_causals = naive_forecasting(Train = products_Train,Test = products_Test)\n",
    "\n",
    "product_naive_test_causals = product_naive_test_causals.reset_index()\n",
    "product_naive_test_causals = product_naive_test_causals[['index','product_code','naive']]\n",
    "product_naive_test_causals.columns = ['date','product_code','product_causal']\n",
    "\n",
    "# List of site code to be forecasted\n",
    "Forecast_ID = np.unique(products['product_code'])\n",
    "\n",
    "#Create Forecast dataframe from Oct'19 to Dec'19   \n",
    "Forecast = pd.DataFrame()\n",
    "\n",
    "for i in Forecast_ID:\n",
    "    # Date range from Oct'19 to Dec'19\n",
    "    date = pd.date_range('10/1/2019', freq='MS', periods=3)\n",
    "    a = pd.DataFrame(i, index=date, columns=['product_code'])\n",
    "    a['date'] = a.index\n",
    "    a = a.reset_index(drop=True)\n",
    "    a['value'] = 0\n",
    "    Forecast = Forecast.append(a)\n",
    "\n",
    "#Naive Forecasting \n",
    "def naive_forecasting(Train,Forecast):\n",
    "    '''Naive Forecasting \n",
    "    '''    \n",
    "    # initialize empty dataset\n",
    "    columns = ['date','product_code','product_causal']\n",
    "    datas = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # select unique site_code\n",
    "    batch_ids = Train['product_code'].unique()\n",
    "\n",
    "    # groupby different site_code\n",
    "    Train = Train.groupby(['product_code'])\n",
    "    Forecast = Forecast.groupby(['product_code'])\n",
    "    \n",
    "    #loop through the batch_ids\n",
    "    for batch_id in batch_ids:\n",
    "        d_train = Train.get_group(batch_id)\n",
    "        test_results = Forecast.get_group(batch_id)\n",
    "        naive_train = np.asarray(d_train.value)\n",
    "        test_results['product_causal'] = naive_train[len(naive_train)-1]\n",
    "        datas = datas.append(test_results)\n",
    "    \n",
    "    return datas\n",
    "\n",
    "products_Train = products_Train.reset_index()\n",
    "products_train = products_Train[['product_code','date','value']]\n",
    "products_Forecast = Forecast[['product_code','date']]\n",
    "product_naive_Forecast_causals = naive_forecasting(products_train,products_Forecast)\n",
    "\n",
    "product_train_causals = products_Train\n",
    "product_train_causals.columns = ['date','product_code','product_causal']\n",
    "## output - product_naive_Forecast_causals, product_naive_test_causals,product_train_causals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Site Level Forecasting - Use as a causal to fit machine learning algorithm ########\n",
    "\n",
    "#site level aggregation\n",
    "site = logistics_df.groupby(['site_code','date']).sum().reset_index()\n",
    "\n",
    "# Split Train Test data: Forecast Horizon is 3, so last 3 data points from each group kept as test data\n",
    "site_Test = site.groupby('site_code').tail(3)\n",
    "\n",
    "# Remove the Test data sets from Train data\n",
    "comb = site_Test['site_code']+site_Test['date'].apply(str)\n",
    "site['conc'] = site['site_code']+site['date'].apply(str)\n",
    "\n",
    "site_Train = site[~site['conc'].isin(comb)]\n",
    "\n",
    "# Drop concatenate column\n",
    "site_Train = site_Train.drop(columns = ['conc'],axis =1)\n",
    "\n",
    "site_Train = site_Train[['site_code','date','value']]\n",
    "site_Test = site_Test[['site_code','date','value']]\n",
    "\n",
    "#Naive Forecasting \n",
    "def naive_forecasting(Train,Test):\n",
    "    '''Naive Forecasting \n",
    "    '''    \n",
    "    # initialize empty dataset\n",
    "    columns = ['site_code','MAE']\n",
    "    datas = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # select unique batch_id\n",
    "    batch_ids = Train['site_code'].unique()\n",
    "\n",
    "    # groupby different batch_id\n",
    "    Train = Train.groupby(['site_code'])\n",
    "    Test = Test.groupby(['site_code'])\n",
    "    \n",
    "    #loop through the batch_ids\n",
    "    for batch_id in batch_ids:\n",
    "        d_train = Train.get_group(batch_id)\n",
    "        d_test = Test.get_group(batch_id)\n",
    "        naive_train = np.asarray(d_train.value)\n",
    "        naive_test = d_test[['site_code','value']]\n",
    "        naive_test['naive'] = naive_train[len(naive_train)-1]\n",
    "        naive_test['MAE'] = abs(naive_test.value - naive_test.naive)\n",
    "        #naive_test['MAE'] = mean_absolute_error(naive_test.value, naive_test.naive)\n",
    "        #d_train = d_train[['batchID','Test','Pred','MAE']]\n",
    "        datas = datas.append(naive_test)\n",
    "    \n",
    "    return datas\n",
    "\n",
    "site_Train = site_Train.set_index('date')\n",
    "site_Test = site_Test.set_index('date')\n",
    "\n",
    "# Fit Naive Forecasting Model \n",
    "site_Train = site_Train[['site_code','value']]\n",
    "site_Test = site_Test[['site_code','value']]\n",
    "\n",
    "site_naive_test_causals = naive_forecasting(Train = site_Train,Test = site_Test)\n",
    "\n",
    "site_naive_test_causals = site_naive_test_causals.reset_index()\n",
    "site_naive_test_causals = site_naive_test_causals[['index','site_code','naive']]\n",
    "site_naive_test_causals.columns = ['date','site_code','site_causal']\n",
    "\n",
    "# List of site code to be forecasted\n",
    "Forecast_ID = np.unique(site['site_code'])\n",
    "\n",
    "#Create Forecast dataframe from Oct'19 to Dec'19   \n",
    "Forecast = pd.DataFrame()\n",
    "\n",
    "for i in Forecast_ID:\n",
    "    # Date range from Oct'19 to Dec'19\n",
    "    date = pd.date_range('10/1/2019', freq='MS', periods=3)\n",
    "    a = pd.DataFrame(i, index=date, columns=['site_code'])\n",
    "    a['date'] = a.index\n",
    "    a = a.reset_index(drop=True)\n",
    "    a['value'] = 0\n",
    "    Forecast = Forecast.append(a)\n",
    "\n",
    "#Naive Forecasting \n",
    "def naive_forecasting(Train,Forecast):\n",
    "    '''Naive Forecasting \n",
    "    '''    \n",
    "    # initialize empty dataset\n",
    "    columns = ['date','site_code','site_causal']\n",
    "    datas = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # select unique site_code\n",
    "    batch_ids = Train['site_code'].unique()\n",
    "\n",
    "    # groupby different site_code\n",
    "    Train = Train.groupby(['site_code'])\n",
    "    Forecast = Forecast.groupby(['site_code'])\n",
    "    \n",
    "    #loop through the batch_ids\n",
    "    for batch_id in batch_ids:\n",
    "        d_train = Train.get_group(batch_id)\n",
    "        test_results = Forecast.get_group(batch_id)\n",
    "        naive_train = np.asarray(d_train.value)\n",
    "        test_results['site_causal'] = naive_train[len(naive_train)-1]\n",
    "        datas = datas.append(test_results)\n",
    "    \n",
    "    return datas\n",
    "\n",
    "site_Train = site_Train.reset_index()\n",
    "site_train = site_Train[['site_code','date','value']]\n",
    "site_Forecast = Forecast[['site_code','date']]\n",
    "site_naive_Forecast_causals = naive_forecasting(site_train,site_Forecast)\n",
    "\n",
    "site_train_causals = site_Train\n",
    "site_train_causals.columns = ['date','site_code','site_causal']\n",
    "## output - site_naive_Forecast_causals, site_naive_test_causals,site_train_causals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test causals - product and site\n",
    "product_causals = pd.concat([product_train_causals,product_naive_test_causals])\n",
    "site_causals = pd.concat([site_train_causals,site_naive_test_causals])\n",
    "\n",
    "# Merge product and site causals with df\n",
    "NonZero_df = pd.merge(NonZero_df,product_causals,how='left',on=['product_code','date'])\n",
    "NonZero_df = pd.merge(NonZero_df,site_causals,how='left',on=['site_code','date'])\n",
    "\n",
    "Forty_Sixty_df = pd.merge(Forty_Sixty_df,product_causals,how='left',on=['product_code','date'])\n",
    "Forty_Sixty_df = pd.merge(Forty_Sixty_df,site_causals,how='left',on=['site_code','date'])\n",
    "\n",
    "Forty_df = pd.merge(Forty_df,product_causals,how='left',on=['product_code','date'])\n",
    "Sixty_df = pd.merge(Sixty_df,product_causals,how='left',on=['product_code','date'])\n",
    "\n",
    "Forty_df = pd.merge(Forty_df,site_causals,how='left',on=['site_code','date'])\n",
    "Sixty_df = pd.merge(Sixty_df,site_causals,how='left',on=['site_code','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add product and site forecast causals to forecast data\n",
    "NonZero_Forecast = pd.merge(NonZero_Forecast,product_naive_Forecast_causals,how='left',on=['product_code','date'])\n",
    "NonZero_Forecast = pd.merge(NonZero_Forecast,site_naive_Forecast_causals,how='left',on=['site_code','date'])\n",
    "\n",
    "Forty_Sixty_df_Forecast = pd.merge(Forty_Sixty_df_Forecast,product_naive_Forecast_causals,how='left',on=['product_code','date'])\n",
    "Forty_Sixty_df_Forecast = pd.merge(Forty_Sixty_df_Forecast,site_naive_Forecast_causals,how='left',on=['site_code','date'])\n",
    "\n",
    "Forty_Forecast = pd.merge(Forty_Forecast,product_naive_Forecast_causals,how='left',on=['product_code','date'])\n",
    "Sixty_Forecast = pd.merge(Sixty_Forecast,product_naive_Forecast_causals,how='left',on=['product_code','date'])\n",
    "\n",
    "Forty_Forecast = pd.merge(Forty_Forecast,site_naive_Forecast_causals,how='left',on=['site_code','date'])\n",
    "Sixty_Forecast = pd.merge(Sixty_Forecast,site_naive_Forecast_causals,how='left',on=['site_code','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by date\n",
    "NonZero_df = NonZero_df.sort_values(by = 'date',ascending=True)\n",
    "Forty_Sixty_df = Forty_Sixty_df.sort_values(by = 'date',ascending=True)\n",
    "Forty_df = Forty_df.sort_values(by = 'date',ascending=True)\n",
    "Sixty_df = Sixty_df.sort_values(by = 'date',ascending=True)\n",
    "\n",
    "# Split data into train and test\n",
    "NonZero_df_Test = NonZero_df.groupby('batchID').tail(3)\n",
    "Forty_Sixty_df_Test = Forty_Sixty_df.groupby('batchID').tail(3)\n",
    "Forty_df_Test = Forty_df.groupby('batchID').tail(3)\n",
    "Sixty_df_Test = Sixty_df.groupby('batchID').tail(3)\n",
    "\n",
    "##################Forty_df#######################\n",
    "# Remove the Test data sets from Train data\n",
    "combn = Forty_df_Test['batchID']+Forty_df_Test['date'].apply(str)\n",
    "Forty_df['conc'] = Forty_df['batchID']+Forty_df['date'].apply(str)\n",
    "\n",
    "Forty_df_Train = Forty_df[~Forty_df['conc'].isin(combn)]\n",
    "\n",
    "# Drop concatenate column\n",
    "Forty_df_Train = Forty_df_Train.drop(columns = ['conc'],axis = 1)\n",
    "\n",
    "\n",
    "################# Forty Sixty_df#######################\n",
    "# Remove the Test data sets from Train data\n",
    "combn = Forty_Sixty_df_Test['batchID']+Forty_Sixty_df_Test['date'].apply(str)\n",
    "Forty_Sixty_df['conc'] = Forty_Sixty_df['batchID']+Forty_Sixty_df['date'].apply(str)\n",
    "\n",
    "Forty_Sixty_df_Train = Forty_Sixty_df[~Forty_Sixty_df['conc'].isin(combn)]\n",
    "\n",
    "# Drop concatenate column\n",
    "Forty_Sixty_df_Train = Forty_Sixty_df_Train.drop(columns = ['conc'],axis = 1)\n",
    "\n",
    "################# Sixty_df#######################\n",
    "# Remove the Test data sets from Train data\n",
    "combn = Sixty_df_Test['batchID']+Sixty_df_Test['date'].apply(str)\n",
    "Sixty_df['conc'] = Sixty_df['batchID']+Sixty_df['date'].apply(str)\n",
    "\n",
    "Sixty_df_Train = Sixty_df[~Sixty_df['conc'].isin(combn)]\n",
    "\n",
    "# Drop concatenate column\n",
    "Sixty_df_Train = Sixty_df_Train.drop(columns = ['conc'],axis = 1)\n",
    "\n",
    "#################NonZero_df###########################\n",
    "# Remove the Test data sets from Train data\n",
    "combn = NonZero_df_Test['batchID']+NonZero_df_Test['date'].apply(str)\n",
    "NonZero_df['conc'] = NonZero_df['batchID']+NonZero_df['date'].apply(str)\n",
    "\n",
    "NonZero_df_Train = NonZero_df[~NonZero_df['conc'].isin(combn)]\n",
    "\n",
    "# Drop concatenate column\n",
    "NonZero_df_Train = NonZero_df_Train.drop(columns = ['conc'],axis = 1)\n",
    "\n",
    "NonZero_df = NonZero_df.drop(columns = ['conc'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Decomposition\n",
    "def decompose(df):\n",
    "    '''Replace out of bound dates batch_id wise \n",
    "    '''\n",
    "    # initialize empty dataset\n",
    "    columns = ['batchID','value']\n",
    "    #datas = [] \n",
    "    datas = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # select unique batch_id\n",
    "    batch_ids = np.unique(df['batchID'])\n",
    "\n",
    "    # groupby different batch_id\n",
    "    df = df.groupby(['batchID'])\n",
    "    \n",
    "    #loop through the batch_ids\n",
    "    for batch_id in batch_ids:\n",
    "        df_partial = df.get_group(batch_id)\n",
    "        try:\n",
    "            result = seasonal_decompose(df_partial.value, model='additive',freq = 12)\n",
    "        except:\n",
    "            continue\n",
    "        df_partial['trend'] = result.trend\n",
    "        df_partial['seasonal'] = result.seasonal\n",
    "        df_partial['resid'] = result.resid\n",
    "        df_partial['observed'] = result.observed\n",
    "        datas = datas.append(df_partial)\n",
    "    \n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# Non Zero_df, Forty_df, Sixty_df\n",
    "NonZero_decomposition = decompose(NonZero_df_Train)\n",
    "Forty_Sixty_df_decomposition = decompose(Forty_Sixty_df_Train)\n",
    "Forty_decomposition = decompose(Forty_df_Train)\n",
    "Sixty_decomposition = decompose(Sixty_df_Train)\n",
    "\n",
    "# Merge Seasonality as causal \n",
    "NonZero_df_Train = pd.merge(NonZero_df_Train,NonZero_decomposition[['batchID','date','seasonal']],how='left',on=['batchID','date']).fillna(0)\n",
    "Forty_Sixty_df_Train = pd.merge(Forty_Sixty_df_Train,Forty_Sixty_df_decomposition[['batchID','date','seasonal']],how='left',on=['batchID','date']).fillna(0)\n",
    "Forty_df_Train = pd.merge(Forty_df_Train,Forty_decomposition[['batchID','date','seasonal']],how='left',on=['batchID','date']).fillna(0)\n",
    "Sixty_df_Train = pd.merge(Sixty_df_Train,Sixty_decomposition[['batchID','date','seasonal']],how='left',on=['batchID','date']).fillna(0)\n",
    "\n",
    "# Seasonal on entire data\n",
    "NonZero_full_decomposition = decompose(NonZero_df)\n",
    "Forty_Sixty_df_full_decomposition = decompose(Forty_Sixty_df)\n",
    "Forty_full_decomposition = decompose(Forty_df)\n",
    "Sixty_full_decomposition = decompose(Sixty_df)\n",
    "\n",
    "# Merge Seasonality as causal \n",
    "NonZero_df = pd.merge(NonZero_df,NonZero_full_decomposition[['batchID','date','seasonal']],how='left',on=['batchID','date']).fillna(0)\n",
    "Forty_Sixty_df = pd.merge(Forty_Sixty_df,Forty_Sixty_df_full_decomposition[['batchID','date','seasonal']],how='left',on=['batchID','date']).fillna(0)\n",
    "Forty_df = pd.merge(Forty_df,Forty_full_decomposition[['batchID','date','seasonal']],how='left',on=['batchID','date']).fillna(0)\n",
    "Sixty_df = pd.merge(Sixty_df,Sixty_full_decomposition[['batchID','date','seasonal']],how='left',on=['batchID','date']).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add seasonal causal to test data and forecast\n",
    "NonZero_seasonal_test = NonZero_df_Train.groupby(['batchID','month'])['seasonal'].mean().reset_index()\n",
    "Forty_Sixty_df_seasonal_test = Forty_Sixty_df_Train.groupby(['batchID','month'])['seasonal'].mean().reset_index()\n",
    "Forty_seasonal_test = Forty_df_Train.groupby(['batchID','month'])['seasonal'].mean().reset_index()\n",
    "Sixty_seasonal_test = Sixty_df_Train.groupby(['batchID','month'])['seasonal'].mean().reset_index()\n",
    "\n",
    "# Add seasonal prediction to test data based on batchID and month\n",
    "NonZero_df_Test = pd.merge(NonZero_df_Test,NonZero_seasonal_test,how='left',on=['batchID','month'])\n",
    "Forty_Sixty_df_Test = pd.merge(Forty_Sixty_df_Test,Forty_Sixty_df_seasonal_test,how='left',on=['batchID','month'])\n",
    "Forty_df_Test = pd.merge(Forty_df_Test,Forty_seasonal_test,how='left',on=['batchID','month'])\n",
    "Sixty_df_Test = pd.merge(Sixty_df_Test,Sixty_seasonal_test,how='left',on=['batchID','month'])\n",
    "\n",
    "#Add Seasonal values to Forecast data\n",
    "NonZero_seasonal = NonZero_df.groupby(['batchID','month'])['seasonal'].mean().reset_index()\n",
    "Forty_Sixty_df_seasonal = Forty_Sixty_df.groupby(['batchID','month'])['seasonal'].mean().reset_index()\n",
    "Forty_seasonal = Forty_df.groupby(['batchID','month'])['seasonal'].mean().reset_index()\n",
    "Sixty_seasonal = Sixty_df.groupby(['batchID','month'])['seasonal'].mean().reset_index()\n",
    "\n",
    "# Add seasonal prediction to test data based on batchID and month\n",
    "NonZero_Forecast = pd.merge(NonZero_Forecast,NonZero_seasonal,how='left',on=['batchID','month'])\n",
    "Forty_Sixty_df_Forecast = pd.merge(Forty_Sixty_df_Forecast,Forty_Sixty_df_seasonal,how='left',on=['batchID','month'])\n",
    "Forty_Forecast = pd.merge(Forty_Forecast,Forty_seasonal,how='left',on=['batchID','month'])\n",
    "Sixty_Forecast = pd.merge(Sixty_Forecast,Sixty_seasonal,how='left',on=['batchID','month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna values with 0\n",
    "Sixty_Forecast = Sixty_Forecast.fillna(0)\n",
    "Forty_Forecast = Forty_Forecast.fillna(0)\n",
    "Forty_Sixty_df_Forecast = Forty_Sixty_df_Forecast.fillna(0)\n",
    "NonZero_Forecast = NonZero_Forecast.fillna(0)\n",
    "\n",
    "# Fillna values with 0\n",
    "Sixty_Forecast = Sixty_Forecast.fillna(0)\n",
    "Forty_Forecast = Forty_Forecast.fillna(0)\n",
    "Forty_Sixty_df_Forecast = Forty_Sixty_df_Forecast.fillna(0)\n",
    "NonZero_Forecast = NonZero_Forecast.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE Forty_Sixty_df -Naive Forecasting: 11.637457044673539\n"
     ]
    }
   ],
   "source": [
    "# Drop date column as it is present in index\n",
    "Forty_Sixty_df_Naive_Train = Forty_Sixty_df_Train.set_index('date')\n",
    "Forty_Sixty_df_Naive_Test = Forty_Sixty_df_Test.set_index('date') \n",
    "\n",
    "Forty_Sixty_df_Naive_Train = Forty_Sixty_df_Naive_Train[['batchID','value']]\n",
    "Forty_Sixty_df_Naive_Test = Forty_Sixty_df_Naive_Test[['batchID','value']]\n",
    "\n",
    "# Fit and predict \n",
    "Forty_Sixty_df_Naive_Forecast = naive_forecasting_test(Train = Forty_Sixty_df_Naive_Train,Test = Forty_Sixty_df_Naive_Test)\n",
    "\n",
    "# Calculate in sample naive forecasting for each group\n",
    "Forty_Sixty_df_Naive_results = Forty_Sixty_df_Naive_Forecast.groupby('batchID')['Naive_MAE'].mean()\n",
    "Forty_Sixty_df_Naive_results = Forty_Sixty_df_Naive_results.reset_index()\n",
    "\n",
    "print(\"Average MAE Forty_Sixty_df -Naive Forecasting:\",Forty_Sixty_df_Naive_results['Naive_MAE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE -Sixty_df -Naive Forecasting: 7.6787878787878805\n"
     ]
    }
   ],
   "source": [
    "# Drop date column as it is present in index\n",
    "Sixty_df_Naive_Train = Sixty_df_Train.set_index('date')\n",
    "Sixty_df_Naive_Test = Sixty_df_Test.set_index('date') \n",
    "\n",
    "Sixty_df_Naive_Train = Sixty_df_Naive_Train[['batchID','value']]\n",
    "Sixty_df_Naive_Test = Sixty_df_Naive_Test[['batchID','value']]\n",
    "\n",
    "# Fit and predict \n",
    "Sixty_df_Naive_Forecast = naive_forecasting_test(Train = Sixty_df_Naive_Train,Test = Sixty_df_Naive_Test)\n",
    "\n",
    "# Calculate in sample naive forecasting for each group\n",
    "Sixty_df_Naive_results = Sixty_df_Naive_Forecast.groupby('batchID')['Naive_MAE'].mean()\n",
    "Sixty_df_Naive_results = Sixty_df_Naive_results.reset_index()\n",
    "\n",
    "print(\"Average MAE -Sixty_df -Naive Forecasting:\",Sixty_df_Naive_results['Naive_MAE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE - Forty_df -Naive Forecasting: 14.566517189835574\n"
     ]
    }
   ],
   "source": [
    "# Drop date column as it is present in index\n",
    "Forty_df_Naive_Train = Forty_df_Train.set_index('date')\n",
    "Forty_df_Naive_Test = Forty_df_Test.set_index('date') \n",
    "\n",
    "# Fit Naive Forecasting Model on Zero df and Less than 5 data points and calculate out of sample MAE\n",
    "Forty_df_Naive_Train = Forty_df_Naive_Train[['batchID','value']]\n",
    "Forty_df_Naive_Test = Forty_df_Naive_Test[['batchID','value']]\n",
    "\n",
    "# Fit and predict \n",
    "Forty_df_Naive_Forecast = naive_forecasting_test(Train = Forty_df_Naive_Train,Test = Forty_df_Naive_Test)\n",
    "\n",
    "# Calculate in sample naive forecasting for each group\n",
    "Forty_df_Naive_results = Forty_df_Naive_Forecast.groupby('batchID')['Naive_MAE'].mean()\n",
    "Forty_df_Naive_results = Forty_df_Naive_results.reset_index()\n",
    "\n",
    "print(\"Average MAE - Forty_df -Naive Forecasting:\",Forty_df_Naive_results['Naive_MAE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE - Zero_df Naive Forecasting: 18.46682653876898\n"
     ]
    }
   ],
   "source": [
    "# Drop date column as it is present in index\n",
    "NonZero_df_Naive_Train = NonZero_df_Train.set_index('date')\n",
    "NonZero_df_Naive_Test = NonZero_df_Test.set_index('date') \n",
    "\n",
    "# Fit Naive Forecasting Model on Zero df and Less than 5 data points and calculate out of sample MAE\n",
    "NonZero_df_Naive_Train = NonZero_df_Naive_Train[['batchID','value']]\n",
    "NonZero_df_Naive_Test = NonZero_df_Naive_Test[['batchID','value']]\n",
    "\n",
    "# Fit and predict \n",
    "NonZero_df_Naive_Forecast = naive_forecasting_test(Train = NonZero_df_Naive_Train,Test = NonZero_df_Naive_Test)\n",
    "\n",
    "# Calculate in sample naive forecasting for each group\n",
    "NonZero_df_Naive_results = NonZero_df_Naive_Forecast.groupby('batchID')['Naive_MAE'].mean()\n",
    "NonZero_df_Naive_results = NonZero_df_Naive_results.reset_index()\n",
    "\n",
    "print(\"Average MAE - Zero_df Naive Forecasting:\",NonZero_df_Naive_results['Naive_MAE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intermittent Forecasting \n",
    "def Croston_Intermittent_forecasting_test(Train,Test):\n",
    "    '''Intermitten Forecasting - Using Croston's method \n",
    "    '''    \n",
    "    # initialize empty dataset\n",
    "    columns = ['batchID','Croston_MAE']\n",
    "    datas = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # select unique batch_id\n",
    "    batch_ids = Train['batchID'].unique()\n",
    "\n",
    "    # groupby different batch_id\n",
    "    Train = Train.groupby(['batchID'])\n",
    "    Test = Test.groupby(['batchID'])\n",
    "    \n",
    "    #loop through the batch_ids\n",
    "    for batch_id in batch_ids:\n",
    "        d_train = Train.get_group(batch_id)\n",
    "        d_test = Test.get_group(batch_id)\n",
    "        int_train = np.asarray(d_train.value)\n",
    "        pred = croston.fit_croston(int_train,3,'original')['croston_forecast']\n",
    "        int_test = d_test[['batchID','value']]\n",
    "        int_test['forecast'] = pred\n",
    "        int_test['Croston_MAE'] = abs(int_test.value - int_test.forecast)\n",
    "        datas = datas.append(int_test)\n",
    "    \n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Croston's Intermittent Forecasting: 17.22281179843117\n",
      "Croston's Intermittent Forecasting: 12.098562325086325\n",
      "Croston's Intermittent Forecasting: 9.420381537240951\n",
      "Croston's Intermittent Forecasting: 14.080175912505519\n"
     ]
    }
   ],
   "source": [
    "from croston import croston\n",
    "# Non Zero_df Croston\n",
    "NonZero_Croston = Croston_Intermittent_forecasting_test(NonZero_df_Train,NonZero_df_Test)\n",
    "#NonZero_Croston = NonZero_Croston.fillna(0)\n",
    "print(\"Croston's Intermittent Forecasting:\",NonZero_Croston['Croston_MAE'].mean())\n",
    "NonZero_Croston_MAE = NonZero_Croston.groupby('batchID')['Croston_MAE'].mean()\n",
    "\n",
    "# Sixty df Croston\n",
    "Forty_Sixty_df_Croston = Croston_Intermittent_forecasting_test(Forty_Sixty_df_Train,Forty_Sixty_df_Test)\n",
    "#Forty_Sixty_df_Croston = Forty_Sixty_df_Croston.fillna(0)\n",
    "print(\"Croston's Intermittent Forecasting:\",Forty_Sixty_df_Croston['Croston_MAE'].mean())\n",
    "Forty_Sixty_df_Croston_MAE = Forty_Sixty_df_Croston.groupby('batchID')['Croston_MAE'].mean()\n",
    "\n",
    "# Sixty df Croston\n",
    "Sixty_Croston = Croston_Intermittent_forecasting_test(Sixty_df_Train,Sixty_df_Test)\n",
    "#Sixty_Croston = Sixty_Croston.fillna(0)\n",
    "print(\"Croston's Intermittent Forecasting:\",Sixty_Croston['Croston_MAE'].mean())\n",
    "Sixty_Croston_MAE = Sixty_Croston.groupby('batchID')['Croston_MAE'].mean()\n",
    "\n",
    "# Forty df Croston\n",
    "Forty_Croston = Croston_Intermittent_forecasting_test(Forty_df_Train,Forty_df_Test)\n",
    "#Forty_Croston = Forty_Croston.fillna(0)\n",
    "print(\"Croston's Intermittent Forecasting:\",Forty_Croston['Croston_MAE'].mean())\n",
    "Forty_Croston_MAE = Forty_Croston.groupby('batchID')['Croston_MAE'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Non Zero df - Calculate lags ##################\n",
    "\n",
    "NonZero_df_Train['tag'] = 'Train'\n",
    "NonZero_df_Test['tag'] = 'Test'\n",
    "NonZero_Forecast['tag'] = 'Forecast'\n",
    "\n",
    "#Combine train and test to calculate demand lags and SMA lags\n",
    "NonZero_df_lag = pd.concat([NonZero_df_Train,NonZero_df_Test,NonZero_Forecast])\n",
    "\n",
    "#Add demand lags \n",
    "for i in range(3, 8):\n",
    "    NonZero_df_lag[\"dm_lag_{}\".format(i)] = NonZero_df_lag.groupby(['batchID'])['value'].shift(i)\n",
    "\n",
    "# Add average monthly consumption lags\n",
    "for i in range(3, 5):\n",
    "    NonZero_df_lag[\"avg_conmptn_lag_{}\".format(i)] = NonZero_df_lag.groupby(['batchID'])['average_monthly_consumption'].shift(i)\n",
    "        \n",
    "# Add opening stock lags\n",
    "for i in range(3, 5):\n",
    "    NonZero_df_lag[\"Stock_Initial_lag{}\".format(i)] = NonZero_df_lag.groupby(['batchID'])['stock_initial'].shift(i)\n",
    "\n",
    "# Calculate SMA lags\n",
    "NonZero_df_lag['SMA_6'] = (NonZero_df_lag['dm_lag_3']+NonZero_df_lag['dm_lag_4']+NonZero_df_lag['dm_lag_5'])/3\n",
    "NonZero_df_lag['SMA_7'] = (NonZero_df_lag['dm_lag_4']+NonZero_df_lag['dm_lag_5']+NonZero_df_lag['dm_lag_6'])/3\n",
    "NonZero_df_lag['SMA_8'] = (NonZero_df_lag['dm_lag_5']+NonZero_df_lag['dm_lag_6']+NonZero_df_lag['dm_lag_7'])/3\n",
    "\n",
    "#drop demand lags, average monthly consumption, initial stock columns\n",
    "NonZero_df_lag = NonZero_df_lag.drop(columns = ['average_monthly_consumption','stock_initial','product_type','site_type','region'],axis=1)\n",
    "\n",
    "# Split NonZero_df_lag into Train, test and forecast\n",
    "NonZero_df_Train = NonZero_df_lag[NonZero_df_lag['tag']=='Train']\n",
    "NonZero_df_Test = NonZero_df_lag[NonZero_df_lag['tag']=='Test']\n",
    "NonZero_Forecast = NonZero_df_lag[NonZero_df_lag['tag']=='Forecast']\n",
    "\n",
    "#Remove tag column\n",
    "NonZero_df_Train = NonZero_df_Train.drop(columns = ['tag'],axis = 1)\n",
    "NonZero_df_Test = NonZero_df_Test.drop(columns = ['tag'],axis = 1)\n",
    "NonZero_Forecast = NonZero_Forecast.drop(columns = ['tag'],axis = 1)\n",
    "\n",
    "NonZero_df_Train = NonZero_df_Train.fillna(0)\n",
    "NonZero_df_Test = NonZero_df_Test.fillna(0)\n",
    "NonZero_Forecast = NonZero_Forecast.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Forty Sixty df - Calculate lags ##################\n",
    "\n",
    "Forty_Sixty_df_Train['tag'] = 'Train'\n",
    "Forty_Sixty_df_Test['tag'] = 'Test'\n",
    "Forty_Sixty_df_Forecast['tag'] = 'Forecast'\n",
    "\n",
    "#Combine train and test to calculate demand lags and SMA lags\n",
    "Forty_Sixty_df_lag = pd.concat([Forty_Sixty_df_Train,Forty_Sixty_df_Test,Forty_Sixty_df_Forecast])\n",
    "\n",
    "#Add demand lags \n",
    "for i in range(3, 8):\n",
    "    Forty_Sixty_df_lag[\"dm_lag_{}\".format(i)] = Forty_Sixty_df_lag.groupby(['batchID'])['value'].shift(i)\n",
    "\n",
    "# Add average monthly consumption lags\n",
    "for i in range(3, 5):\n",
    "    Forty_Sixty_df_lag[\"avg_conmptn_lag_{}\".format(i)] = Forty_Sixty_df_lag.groupby(['batchID'])['average_monthly_consumption'].shift(i)\n",
    "        \n",
    "# Add opening stock lags\n",
    "for i in range(3, 5):\n",
    "    Forty_Sixty_df_lag[\"Stock_Initial_lag{}\".format(i)] = Forty_Sixty_df_lag.groupby(['batchID'])['stock_initial'].shift(i)\n",
    "\n",
    "# Calculate SMA lags\n",
    "Forty_Sixty_df_lag['SMA_6'] = (Forty_Sixty_df_lag['dm_lag_3']+Forty_Sixty_df_lag['dm_lag_4']+Forty_Sixty_df_lag['dm_lag_5'])/3\n",
    "Forty_Sixty_df_lag['SMA_7'] = (Forty_Sixty_df_lag['dm_lag_4']+Forty_Sixty_df_lag['dm_lag_5']+Forty_Sixty_df_lag['dm_lag_6'])/3\n",
    "Forty_Sixty_df_lag['SMA_8'] = (Forty_Sixty_df_lag['dm_lag_5']+Forty_Sixty_df_lag['dm_lag_6']+Forty_Sixty_df_lag['dm_lag_7'])/3\n",
    "\n",
    "#drop demand lags, average monthly consumption, initial stock columns\n",
    "Forty_Sixty_df_lag = Forty_Sixty_df_lag.drop(columns = ['average_monthly_consumption','stock_initial','product_type','site_type','region'],axis=1)\n",
    "\n",
    "# Split NonZero_df_lag into Train, test and forecast\n",
    "Forty_Sixty_df_Train = Forty_Sixty_df_lag[Forty_Sixty_df_lag['tag']=='Train']\n",
    "Forty_Sixty_df_Test = Forty_Sixty_df_lag[Forty_Sixty_df_lag['tag']=='Test']\n",
    "Forty_Sixty_df_Forecast = Forty_Sixty_df_lag[Forty_Sixty_df_lag['tag']=='Forecast']\n",
    "\n",
    "#Remove tag column\n",
    "Forty_Sixty_df_Train = Forty_Sixty_df_Train.drop(columns = ['tag'],axis = 1)\n",
    "Forty_Sixty_df_Test = Forty_Sixty_df_Test.drop(columns = ['tag'],axis = 1)\n",
    "Forty_Sixty_df_Forecast = Forty_Sixty_df_Forecast.drop(columns = ['tag'],axis = 1)\n",
    "\n",
    "Forty_Sixty_df_Train = Forty_Sixty_df_Train.fillna(0)\n",
    "Forty_Sixty_df_Test = Forty_Sixty_df_Test.fillna(0)\n",
    "Forty_Sixty_df_Forecast = Forty_Sixty_df_Forecast.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Sixty df - Calculate lags ##################\n",
    "\n",
    "Sixty_df_Train['tag'] = 'Train'\n",
    "Sixty_df_Test['tag'] = 'Test'\n",
    "Sixty_Forecast['tag'] = 'Forecast'\n",
    "\n",
    "#Combine train and test to calculate demand lags and SMA lags\n",
    "Sixty_df_lag = pd.concat([Sixty_df_Train,Sixty_df_Test,Sixty_Forecast])\n",
    "\n",
    "#Add demand lags \n",
    "for i in range(3, 8):\n",
    "    Sixty_df_lag[\"dm_lag_{}\".format(i)] = Sixty_df_lag.groupby(['batchID'])['value'].shift(i)\n",
    "\n",
    "# Add average monthly consumption lags\n",
    "for i in range(3, 5):\n",
    "    Sixty_df_lag[\"avg_conmptn_lag_{}\".format(i)] = Sixty_df_lag.groupby(['batchID'])['average_monthly_consumption'].shift(i)\n",
    "        \n",
    "# Add opening stock lags\n",
    "for i in range(3, 5):\n",
    "    Sixty_df_lag[\"Stock_Initial_lag{}\".format(i)] = Sixty_df_lag.groupby(['batchID'])['stock_initial'].shift(i)\n",
    "\n",
    "# Calculate SMA lags\n",
    "Sixty_df_lag['SMA_6'] = (Sixty_df_lag['dm_lag_3']+Sixty_df_lag['dm_lag_4']+Sixty_df_lag['dm_lag_5'])/3\n",
    "Sixty_df_lag['SMA_7'] = (Sixty_df_lag['dm_lag_4']+Sixty_df_lag['dm_lag_5']+Sixty_df_lag['dm_lag_6'])/3\n",
    "Sixty_df_lag['SMA_8'] = (Sixty_df_lag['dm_lag_5']+Sixty_df_lag['dm_lag_6']+Sixty_df_lag['dm_lag_7'])/3\n",
    "\n",
    "#drop demand lags, average monthly consumption, initial stock columns\n",
    "Sixty_df_lag = Sixty_df_lag.drop(columns = ['average_monthly_consumption','stock_initial','product_type','site_type','region'],axis=1)\n",
    "\n",
    "# Split NonZero_df_lag into Train, test and forecast\n",
    "Sixty_df_Train = Sixty_df_lag[Sixty_df_lag['tag']=='Train']\n",
    "Sixty_df_Test = Sixty_df_lag[Sixty_df_lag['tag']=='Test']\n",
    "Sixty_Forecast = Sixty_df_lag[Sixty_df_lag['tag']=='Forecast']\n",
    "\n",
    "#Remove tag column\n",
    "Sixty_df_Train = Sixty_df_Train.drop(columns = ['tag'],axis = 1)\n",
    "Sixty_df_Test = Sixty_df_Test.drop(columns = ['tag'],axis = 1)\n",
    "Sixty_Forecast = Sixty_Forecast.drop(columns = ['tag'],axis = 1)\n",
    "\n",
    "Sixty_df_Train = Sixty_df_Train.fillna(0)\n",
    "Sixty_df_Test = Sixty_df_Test.fillna(0)\n",
    "Sixty_Forecast = Sixty_Forecast.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Forty df - Calculate lags ##################\n",
    "\n",
    "Forty_df_Train['tag'] = 'Train'\n",
    "Forty_df_Test['tag'] = 'Test'\n",
    "Forty_Forecast['tag'] = 'Forecast'\n",
    "\n",
    "#Combine train and test to calculate demand lags and SMA lags\n",
    "Forty_df_lag = pd.concat([Forty_df_Train,Forty_df_Test,Forty_Forecast])\n",
    "\n",
    "#Add demand lags \n",
    "for i in range(3, 8):\n",
    "    Forty_df_lag[\"dm_lag_{}\".format(i)] = Forty_df_lag.groupby(['batchID'])['value'].shift(i)\n",
    "\n",
    "# Add average monthly consumption lags\n",
    "for i in range(3, 5):\n",
    "    Forty_df_lag[\"avg_conmptn_lag_{}\".format(i)] = Forty_df_lag.groupby(['batchID'])['average_monthly_consumption'].shift(i)\n",
    "        \n",
    "# Add opening stock lags\n",
    "for i in range(3, 5):\n",
    "    Forty_df_lag[\"Stock_Initial_lag{}\".format(i)] = Forty_df_lag.groupby(['batchID'])['stock_initial'].shift(i)\n",
    "\n",
    "# Calculate SMA lags\n",
    "Forty_df_lag['SMA_6'] = (Forty_df_lag['dm_lag_3']+Forty_df_lag['dm_lag_4']+Forty_df_lag['dm_lag_5'])/3\n",
    "Forty_df_lag['SMA_7'] = (Forty_df_lag['dm_lag_4']+Forty_df_lag['dm_lag_5']+Forty_df_lag['dm_lag_6'])/3\n",
    "Forty_df_lag['SMA_8'] = (Forty_df_lag['dm_lag_5']+Forty_df_lag['dm_lag_6']+Forty_df_lag['dm_lag_7'])/3\n",
    "\n",
    "#drop demand lags, average monthly consumption, initial stock columns\n",
    "Forty_df_lag = Forty_df_lag.drop(columns = ['average_monthly_consumption','stock_initial','product_type','site_type','region'],axis=1)\n",
    "\n",
    "# Split NonZero_df_lag into Train, test and forecast\n",
    "Forty_df_Train = Forty_df_lag[Forty_df_lag['tag']=='Train']\n",
    "Forty_df_Test = Forty_df_lag[Forty_df_lag['tag']=='Test']\n",
    "Forty_Forecast = Forty_df_lag[Forty_df_lag['tag']=='Forecast']\n",
    "\n",
    "#Remove tag column\n",
    "Forty_df_Train = Forty_df_Train.drop(columns = ['tag'],axis = 1)\n",
    "Forty_df_Test = Forty_df_Test.drop(columns = ['tag'],axis = 1)\n",
    "Forty_Forecast = Forty_Forecast.drop(columns = ['tag'],axis = 1)\n",
    "\n",
    "Forty_df_Train = Forty_df_Train.fillna(0)\n",
    "Forty_df_Test = Forty_df_Test.fillna(0)\n",
    "Forty_Forecast = Forty_Forecast.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features from object to category to fit light gbm model\n",
    "NonZero_X_Forecast = NonZero_Forecast\n",
    "NonZero_X_Forecast = NonZero_X_Forecast.set_index('date')\n",
    "NonZero_X_Forecast = NonZero_X_Forecast.drop(columns=['value'],axis=1)\n",
    "\n",
    "for c in NonZero_X_Forecast.columns:\n",
    "    col_type = NonZero_X_Forecast[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        NonZero_X_Forecast[c] = NonZero_X_Forecast[c].astype('category')\n",
    "\n",
    "Forty_Sixty_df_X_Forecast = Forty_Sixty_df_Forecast\n",
    "Forty_Sixty_df_X_Forecast = Forty_Sixty_df_X_Forecast.set_index('date')\n",
    "Forty_Sixty_df_X_Forecast = Forty_Sixty_df_X_Forecast.drop(columns=['value'],axis=1)\n",
    "\n",
    "for c in Forty_Sixty_df_X_Forecast.columns:\n",
    "    col_type = Forty_Sixty_df_X_Forecast[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Forty_Sixty_df_X_Forecast[c] = Forty_Sixty_df_X_Forecast[c].astype('category')\n",
    "\n",
    "Forty_X_Forecast = Forty_Forecast\n",
    "Forty_X_Forecast = Forty_X_Forecast.set_index('date')\n",
    "Forty_X_Forecast = Forty_X_Forecast.drop(columns=['value'],axis=1)\n",
    "\n",
    "for c in Forty_X_Forecast.columns:\n",
    "    col_type = Forty_X_Forecast[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Forty_X_Forecast[c] = Forty_X_Forecast[c].astype('category')\n",
    "\n",
    "Sixty_X_Forecast = Sixty_Forecast\n",
    "Sixty_X_Forecast = Sixty_X_Forecast.set_index('date')\n",
    "Sixty_X_Forecast = Sixty_X_Forecast.drop(columns=['value'],axis=1)\n",
    "\n",
    "for c in Sixty_X_Forecast.columns:\n",
    "    col_type = Sixty_X_Forecast[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Sixty_X_Forecast[c] = Sixty_X_Forecast[c].astype('category')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Split Train and Test into features and target variable ####\n",
    "\n",
    "# Non Zero df Train and Test\n",
    "NonZero_X_Train = NonZero_df_Train.drop(columns=['value','date'],axis = 1)\n",
    "NonZero_y_Train = NonZero_df_Train[['batchID','value']]\n",
    "\n",
    "NonZero_X_Test = NonZero_df_Test.drop(columns = ['value','date'],axis = 1)\n",
    "NonZero_y_Test = NonZero_df_Test[['batchID','value']]\n",
    "\n",
    "# Convert the categorical features from object type to category to fit lightgbm model\n",
    "for c in NonZero_X_Train.columns:\n",
    "    col_type = NonZero_X_Train[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        NonZero_X_Train[c] = NonZero_X_Train[c].astype('category')\n",
    "        \n",
    "for c in NonZero_X_Test.columns:\n",
    "    col_type = NonZero_X_Test[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        NonZero_X_Test[c] = NonZero_X_Test[c].astype('category')\n",
    "\n",
    "# Combine Train and test to fit on entire data\n",
    "NonZero_full_X_train = pd.concat([NonZero_X_Train,NonZero_X_Test])\n",
    "NonZero_full_y_train = pd.concat([NonZero_y_Train,NonZero_y_Test])\n",
    "\n",
    "# Forty df Train and Test\n",
    "Forty_X_Train = Forty_df_Train.drop(columns=['value','date'],axis = 1)\n",
    "Forty_y_Train = Forty_df_Train[['batchID','value']]\n",
    "\n",
    "Forty_X_Test = Forty_df_Test.drop(columns = ['value','date'],axis = 1)\n",
    "Forty_y_Test = Forty_df_Test[['batchID','value']]\n",
    "\n",
    "# Convert the categorical features from object type to category to fit lightgbm model\n",
    "for c in Forty_X_Train.columns:\n",
    "    col_type = Forty_X_Train[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Forty_X_Train[c] = Forty_X_Train[c].astype('category')\n",
    "        \n",
    "for c in Forty_X_Test.columns:\n",
    "    col_type = Forty_X_Test[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Forty_X_Test[c] = Forty_X_Test[c].astype('category')\n",
    "\n",
    "# Combine Train and test to fit on entire data\n",
    "Forty_full_X_train = pd.concat([Forty_X_Train,Forty_X_Test])\n",
    "Forty_full_y_train = pd.concat([Forty_y_Train,Forty_y_Test])\n",
    "                \n",
    "# Sixty df Train and Test\n",
    "Sixty_X_Train = Sixty_df_Train.drop(columns=['value','date'],axis = 1)\n",
    "Sixty_y_Train = Sixty_df_Train[['batchID','value']]\n",
    "\n",
    "Sixty_X_Test = Sixty_df_Test.drop(columns = ['value','date'],axis = 1)\n",
    "Sixty_y_Test = Sixty_df_Test[['batchID','value']]\n",
    "\n",
    "# Convert the categorical features from object type to category to fit lightgbm model\n",
    "for c in Sixty_X_Train.columns:\n",
    "    col_type = Sixty_X_Train[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Sixty_X_Train[c] = Sixty_X_Train[c].astype('category')\n",
    "        \n",
    "for c in Sixty_X_Test.columns:\n",
    "    col_type = Sixty_X_Test[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Sixty_X_Test[c] = Sixty_X_Test[c].astype('category')\n",
    "\n",
    "# Combine Train and test to fit on entire data\n",
    "Sixty_full_X_train = pd.concat([Sixty_X_Train,Sixty_X_Test])\n",
    "Sixty_full_y_train = pd.concat([Sixty_y_Train,Sixty_y_Test])\n",
    "\n",
    "# Forty df Train and Test\n",
    "Forty_Sixty_df_X_Train = Forty_Sixty_df_Train.drop(columns=['value','date'],axis = 1)\n",
    "Forty_Sixty_df_y_Train = Forty_Sixty_df_Train[['batchID','value']]\n",
    "\n",
    "Forty_Sixty_df_X_Test = Forty_Sixty_df_Test.drop(columns = ['value','date'],axis = 1)\n",
    "Forty_Sixty_df_y_Test = Forty_Sixty_df_Test[['batchID','value']]\n",
    "\n",
    "# Convert the categorical features from object type to category to fit lightgbm model\n",
    "for c in Forty_Sixty_df_X_Train.columns:\n",
    "    col_type = Forty_Sixty_df_X_Train[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Forty_Sixty_df_X_Train[c] = Forty_Sixty_df_X_Train[c].astype('category')\n",
    "        \n",
    "for c in Forty_Sixty_df_X_Test.columns:\n",
    "    col_type = Forty_Sixty_df_X_Test[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        Forty_Sixty_df_X_Test[c] = Forty_Sixty_df_X_Test[c].astype('category')\n",
    "\n",
    "# Combine Train and test to fit on entire data\n",
    "Forty_Sixty_full_X_train = pd.concat([Forty_Sixty_df_X_Train,Forty_Sixty_df_X_Test])\n",
    "Forty_Sixty_full_y_train = pd.concat([Forty_Sixty_df_y_Train,Forty_Sixty_df_y_Test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a global catboost model - NonZero\n",
    "NonZero_X_train_gbm = NonZero_X_Train.drop(columns = ['batchID'],axis = 1)\n",
    "NonZero_y_train_gbm = NonZero_y_Train.drop(columns = ['batchID'],axis = 1)\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Create a list of column names\n",
    "cat_cols = NonZero_X_train_gbm.select_dtypes(include=['category'])\n",
    "    \n",
    "categorical_features = [NonZero_X_train_gbm.columns.tolist().index(x) for x in cat_cols]\n",
    "\n",
    "train_pool = Pool(NonZero_X_train_gbm, label=NonZero_y_train_gbm,cat_features=categorical_features)\n",
    "\n",
    "cb_tweedie = CatBoostRegressor(loss_function='Tweedie:variance_power=1.1', n_estimators=500, silent=True,random_seed=153)\n",
    "cb_tweedie.fit(train_pool)#, eval_set=test_pool)\n",
    "\n",
    "#Predict on test data and calculate out of sample Mean absolute error for each batchID group\n",
    "# select unique batch_id\n",
    "batch_ids = NonZero_X_Test['batchID'].unique()\n",
    "\n",
    "catboost_mae = []\n",
    "batchID = []\n",
    "result_dict = {}\n",
    "NonZero_Catboost_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    df_X_test = NonZero_X_Test[NonZero_X_Test['batchID'] == i]\n",
    "    df_X_test = df_X_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    df_y_test = test_results = NonZero_y_Test[NonZero_y_Test['batchID'] == i]\n",
    "    df_y_test = df_y_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    test_pool = Pool(df_X_test,cat_features=categorical_features) #label=df_y_test\n",
    "    \n",
    "    #predict on test data\n",
    "    cb_tweedie_pred = cb_tweedie.predict(test_pool)\n",
    "        \n",
    "    test_results['prediction'] = cb_tweedie_pred\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    NonZero_Catboost_Forecasts = NonZero_Catboost_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(df_y_test, cb_tweedie_pred)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    catboost_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "result_dict = {'batchID':batchID,\n",
    "               'MAE':catboost_mae}\n",
    "\n",
    "NonZero_catboost_MAE = pd.DataFrame(result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a global lightgbm model\n",
    "NonZero_X_train_gbm = NonZero_X_Train.drop(columns = ['batchID'],axis = 1)\n",
    "NonZero_y_train_gbm = NonZero_y_Train.drop(columns = ['batchID'],axis = 1)\n",
    "\n",
    "#Light gbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Fit light gbm with tweedie loss function\n",
    "model = lgb.LGBMRegressor(objective=\"tweedie\",random_state=153,tweedie_variance_power = 1.1)\n",
    "model.fit(NonZero_X_train_gbm, NonZero_y_train_gbm)\n",
    "\n",
    "#Predict on test data and calculate out of sample Mean absolute error for each batchID group\n",
    "# select unique batch_id\n",
    "batch_ids = NonZero_X_Test['batchID'].unique()\n",
    "\n",
    "lightgbm_mae = []\n",
    "batchID = []\n",
    "result_dict = {}\n",
    "NonZero_lightgbm_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    df_X_test = NonZero_X_Test[NonZero_X_Test['batchID'] == i]\n",
    "    df_X_test = df_X_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    df_y_test = test_results = NonZero_y_Test[NonZero_y_Test['batchID'] == i]\n",
    "    df_y_test = df_y_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    #predict on test data\n",
    "    preds = model.predict(df_X_test)\n",
    "        \n",
    "    test_results['prediction'] = preds\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    NonZero_lightgbm_Forecasts = NonZero_lightgbm_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(df_y_test, preds)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    lightgbm_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "result_dict = {'batchID':batchID,\n",
    "               'MAE':lightgbm_mae}\n",
    "\n",
    "NonZero_lightgbm_MAE = pd.DataFrame(result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a global catboost model - NonZero\n",
    "Forty_Sixty_df_X_train_gbm = Forty_Sixty_df_X_Train.drop(columns = ['batchID'],axis = 1)\n",
    "Forty_Sixty_df_y_train_gbm = Forty_Sixty_df_y_Train.drop(columns = ['batchID'],axis = 1)\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Create a list of column names\n",
    "cat_cols = Forty_Sixty_df_X_train_gbm.select_dtypes(include=['category'])\n",
    "    \n",
    "categorical_features = [Forty_Sixty_df_X_train_gbm.columns.tolist().index(x) for x in cat_cols]\n",
    "\n",
    "train_pool = Pool(Forty_Sixty_df_X_train_gbm, label=Forty_Sixty_df_y_train_gbm,cat_features=categorical_features)\n",
    "\n",
    "cb_tweedie = CatBoostRegressor(loss_function='Tweedie:variance_power=1.1', n_estimators=500, silent=True,random_seed=153)\n",
    "cb_tweedie.fit(train_pool)#, eval_set=test_pool)\n",
    "\n",
    "#Predict on test data and calculate out of sample Mean absolute error for each batchID group\n",
    "# select unique batch_id\n",
    "batch_ids = Forty_Sixty_df_X_Test['batchID'].unique()\n",
    "\n",
    "catboost_mae = []\n",
    "batchID = []\n",
    "result_dict = {}\n",
    "Forty_Sixty_df_Catboost_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    df_X_test = Forty_Sixty_df_X_Test[Forty_Sixty_df_X_Test['batchID'] == i]\n",
    "    df_X_test = df_X_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    df_y_test = test_results = Forty_Sixty_df_y_Test[Forty_Sixty_df_y_Test['batchID'] == i]\n",
    "    df_y_test = df_y_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    test_pool = Pool(df_X_test,cat_features=categorical_features) #label=df_y_test\n",
    "    \n",
    "    #predict on test data\n",
    "    cb_tweedie_pred = cb_tweedie.predict(test_pool)\n",
    "        \n",
    "    test_results['prediction'] = cb_tweedie_pred\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Forty_Sixty_df_Catboost_Forecasts = Forty_Sixty_df_Catboost_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(df_y_test, cb_tweedie_pred)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    catboost_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "result_dict = {'batchID':batchID,\n",
    "               'MAE':catboost_mae}\n",
    "\n",
    "Forty_Sixty_df_catboost_MAE = pd.DataFrame(result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a global lightgbm model\n",
    "Forty_Sixty_df_X_train_gbm = Forty_Sixty_df_X_Train.drop(columns = ['batchID'],axis = 1)\n",
    "Forty_Sixty_df_y_train_gbm = Forty_Sixty_df_y_Train.drop(columns = ['batchID'],axis = 1)\n",
    "\n",
    "#Light gbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Fit light gbm with tweedie loss function\n",
    "model = lgb.LGBMRegressor(objective=\"tweedie\",random_state=153,tweedie_variance_power = 1.1)\n",
    "model.fit(Forty_Sixty_df_X_train_gbm, Forty_Sixty_df_y_train_gbm)\n",
    "\n",
    "#Predict on test data and calculate out of sample Mean absolute error for each batchID group\n",
    "# select unique batch_id\n",
    "batch_ids = Forty_Sixty_df_X_Test['batchID'].unique()\n",
    "\n",
    "lightgbm_mae = []\n",
    "batchID = []\n",
    "result_dict = {}\n",
    "Forty_Sixty_df_lightgbm_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    df_X_test = Forty_Sixty_df_X_Test[Forty_Sixty_df_X_Test['batchID'] == i]\n",
    "    df_X_test = df_X_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    df_y_test = test_results = Forty_Sixty_df_y_Test[Forty_Sixty_df_y_Test['batchID'] == i]\n",
    "    df_y_test = df_y_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    #predict on test data\n",
    "    preds = model.predict(df_X_test)\n",
    "        \n",
    "    test_results['prediction'] = preds\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Forty_Sixty_df_lightgbm_Forecasts = Forty_Sixty_df_lightgbm_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(df_y_test, preds)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    lightgbm_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "result_dict = {'batchID':batchID,\n",
    "               'MAE':lightgbm_mae}\n",
    "\n",
    "Forty_Sixty_df_lightgbm_MAE = pd.DataFrame(result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a global catboost model - NonZero\n",
    "Forty_X_train_gbm = Forty_X_Train.drop(columns = ['batchID'],axis = 1)\n",
    "Forty_y_train_gbm = Forty_y_Train.drop(columns = ['batchID'],axis = 1)\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Create a list of column names\n",
    "cat_cols = Forty_X_train_gbm.select_dtypes(include=['category'])\n",
    "    \n",
    "categorical_features = [Forty_X_train_gbm.columns.tolist().index(x) for x in cat_cols]\n",
    "\n",
    "train_pool = Pool(Forty_X_train_gbm, label=Forty_y_train_gbm,cat_features=categorical_features)\n",
    "\n",
    "cb_tweedie = CatBoostRegressor(loss_function='Tweedie:variance_power=1.1', n_estimators=500, silent=True,random_seed=153)\n",
    "cb_tweedie.fit(train_pool)#, eval_set=test_pool)\n",
    "\n",
    "#Predict on test data and calculate out of sample Mean absolute error for each batchID group\n",
    "# select unique batch_id\n",
    "batch_ids = Forty_X_Test['batchID'].unique()\n",
    "\n",
    "catboost_mae = []\n",
    "batchID = []\n",
    "result_dict = {}\n",
    "Forty_Catboost_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    df_X_test = Forty_X_Test[Forty_X_Test['batchID'] == i]\n",
    "    df_X_test = df_X_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    df_y_test = test_results = Forty_y_Test[Forty_y_Test['batchID'] == i]\n",
    "    df_y_test = df_y_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    test_pool = Pool(df_X_test,cat_features=categorical_features) #label=df_y_test\n",
    "    \n",
    "    #predict on test data\n",
    "    cb_tweedie_pred = cb_tweedie.predict(test_pool)\n",
    "        \n",
    "    test_results['prediction'] = cb_tweedie_pred\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Forty_Catboost_Forecasts = Forty_Catboost_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(df_y_test, cb_tweedie_pred)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    catboost_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "result_dict = {'batchID':batchID,\n",
    "               'MAE':catboost_mae}\n",
    "\n",
    "Forty_catboost_MAE = pd.DataFrame(result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a global lightgbm model\n",
    "Forty_X_train_gbm = Forty_X_Train.drop(columns = ['batchID'],axis = 1)\n",
    "Forty_y_train_gbm = Forty_y_Train.drop(columns = ['batchID'],axis = 1)\n",
    "\n",
    "#Light gbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Fit light gbm with tweedie loss function\n",
    "model = lgb.LGBMRegressor(objective=\"tweedie\",random_state=153,tweedie_variance_power = 1.1)\n",
    "model.fit(Forty_X_train_gbm, Forty_y_train_gbm)\n",
    "\n",
    "#Predict on test data and calculate out of sample Mean absolute error for each batchID group\n",
    "# select unique batch_id\n",
    "batch_ids = Forty_X_Test['batchID'].unique()\n",
    "\n",
    "lightgbm_mae = []\n",
    "batchID = []\n",
    "result_dict = {}\n",
    "Forty_lightgbm_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    df_X_test = Forty_X_Test[Forty_X_Test['batchID'] == i]\n",
    "    df_X_test = df_X_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    df_y_test = test_results = Forty_y_Test[Forty_y_Test['batchID'] == i]\n",
    "    df_y_test = df_y_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    #predict on test data\n",
    "    preds = model.predict(df_X_test)\n",
    "        \n",
    "    test_results['prediction'] = preds\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Forty_lightgbm_Forecasts = Forty_lightgbm_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(df_y_test, preds)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    lightgbm_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "result_dict = {'batchID':batchID,\n",
    "               'MAE':lightgbm_mae}\n",
    "\n",
    "Forty_lightgbm_MAE = pd.DataFrame(result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a global catboost model - NonZero\n",
    "Sixty_X_train_gbm = Sixty_X_Train.drop(columns = ['batchID'],axis = 1)\n",
    "Sixty_y_train_gbm = Sixty_y_Train.drop(columns = ['batchID'],axis = 1)\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Create a list of column names\n",
    "cat_cols = Sixty_X_train_gbm.select_dtypes(include=['category'])\n",
    "    \n",
    "categorical_features = [Sixty_X_train_gbm.columns.tolist().index(x) for x in cat_cols]\n",
    "\n",
    "train_pool = Pool(Sixty_X_train_gbm, label=Sixty_y_train_gbm,cat_features=categorical_features)\n",
    "\n",
    "cb_tweedie = CatBoostRegressor(loss_function='Tweedie:variance_power=1.1', n_estimators=500, silent=True,random_seed=153)\n",
    "cb_tweedie.fit(train_pool)#, eval_set=test_pool)\n",
    "\n",
    "#Predict on test data and calculate out of sample Mean absolute error for each batchID group\n",
    "# select unique batch_id\n",
    "batch_ids = Sixty_X_Test['batchID'].unique()\n",
    "\n",
    "catboost_mae = []\n",
    "batchID = []\n",
    "result_dict = {}\n",
    "Sixty_Catboost_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    df_X_test = Sixty_X_Test[Sixty_X_Test['batchID'] == i]\n",
    "    df_X_test = df_X_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    df_y_test = test_results = Sixty_y_Test[Sixty_y_Test['batchID'] == i]\n",
    "    df_y_test = df_y_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    test_pool = Pool(df_X_test,cat_features=categorical_features) #label=df_y_test\n",
    "    \n",
    "    #predict on test data\n",
    "    cb_tweedie_pred = cb_tweedie.predict(test_pool)\n",
    "        \n",
    "    test_results['prediction'] = cb_tweedie_pred\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Sixty_Catboost_Forecasts = Sixty_Catboost_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(df_y_test, cb_tweedie_pred)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    catboost_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "result_dict = {'batchID':batchID,\n",
    "               'MAE':catboost_mae}\n",
    "\n",
    "Sixty_catboost_MAE = pd.DataFrame(result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a global lightgbm model\n",
    "Sixty_X_train_gbm = Sixty_X_Train.drop(columns = ['batchID'],axis = 1)\n",
    "Sixty_y_train_gbm = Sixty_y_Train.drop(columns = ['batchID'],axis = 1)\n",
    "\n",
    "#Light gbm\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Fit light gbm with tweedie loss function\n",
    "model = lgb.LGBMRegressor(objective=\"tweedie\",random_state=153,tweedie_variance_power = 1.1)\n",
    "model.fit(Sixty_X_train_gbm, Sixty_y_train_gbm)\n",
    "\n",
    "#Predict on test data and calculate out of sample Mean absolute error for each batchID group\n",
    "# select unique batch_id\n",
    "batch_ids = Sixty_X_Test['batchID'].unique()\n",
    "\n",
    "lightgbm_mae = []\n",
    "batchID = []\n",
    "result_dict = {}\n",
    "Sixty_lightgbm_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    df_X_test = Sixty_X_Test[Sixty_X_Test['batchID'] == i]\n",
    "    df_X_test = df_X_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    df_y_test = test_results = Sixty_y_Test[Sixty_y_Test['batchID'] == i]\n",
    "    df_y_test = df_y_test.drop(columns= ['batchID'],axis=1)\n",
    "    \n",
    "    #predict on test data\n",
    "    preds = model.predict(df_X_test)\n",
    "        \n",
    "    test_results['prediction'] = preds\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Sixty_lightgbm_Forecasts = Sixty_lightgbm_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(df_y_test, preds)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    lightgbm_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "result_dict = {'batchID':batchID,\n",
    "               'MAE':lightgbm_mae}\n",
    "\n",
    "Sixty_lightgbm_MAE = pd.DataFrame(result_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding \n",
    "cols = ['district','site_code','product_code','year','month','quarter']\n",
    "\n",
    "# NonZero \n",
    "NonZero_X_Train['tag'] = 'Train'\n",
    "NonZero_X_Test['tag'] = 'Test'\n",
    "NonZero_X_Forecast['tag'] = 'Forecast'\n",
    "\n",
    "NonZero_df_rev = pd.concat([NonZero_X_Train,NonZero_X_Test,NonZero_X_Forecast])\n",
    "\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "NonZero_df_rev = pd.get_dummies(NonZero_df_rev,columns=cols,drop_first=True)\n",
    "\n",
    "NonZero_X_Train = NonZero_df_rev[NonZero_df_rev['tag'] == 'Train']\n",
    "NonZero_X_Test = NonZero_df_rev[NonZero_df_rev['tag'] == 'Test']\n",
    "NonZero_X_Forecast = NonZero_df_rev[NonZero_df_rev['tag'] == 'Forecast']\n",
    "\n",
    "## Forty Sixty df\n",
    "Forty_Sixty_df_X_Train['tag'] = 'Train'\n",
    "Forty_Sixty_df_X_Test['tag'] = 'Test'\n",
    "Forty_Sixty_df_X_Forecast['tag'] = 'Forecast'\n",
    "\n",
    "Forty_Sixty_df_rev = pd.concat([Forty_Sixty_df_X_Train,Forty_Sixty_df_X_Test,Forty_Sixty_df_X_Forecast])\n",
    "\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "Forty_Sixty_df_rev = pd.get_dummies(Forty_Sixty_df_rev,columns=cols,drop_first=True)\n",
    "\n",
    "Forty_Sixty_df_X_Train = Forty_Sixty_df_rev[Forty_Sixty_df_rev['tag'] == 'Train']\n",
    "Forty_Sixty_df_X_Test = Forty_Sixty_df_rev[Forty_Sixty_df_rev['tag'] == 'Test']\n",
    "Forty_Sixty_df_X_Forecast = Forty_Sixty_df_rev[Forty_Sixty_df_rev['tag'] == 'Forecast']\n",
    "\n",
    "## Sixty df\n",
    "Sixty_X_Train['tag'] = 'Train'\n",
    "Sixty_X_Test['tag'] = 'Test'\n",
    "Sixty_X_Forecast['tag'] = 'Forecast'\n",
    "\n",
    "Sixty_df_rev = pd.concat([Sixty_X_Train,Sixty_X_Test,Sixty_X_Forecast])\n",
    "\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "Sixty_df_rev = pd.get_dummies(Sixty_df_rev,columns=cols,drop_first=True)\n",
    "\n",
    "Sixty_X_Train = Sixty_df_rev[Sixty_df_rev['tag'] == 'Train']\n",
    "Sixty_X_Test = Sixty_df_rev[Sixty_df_rev['tag'] == 'Test']\n",
    "Sixty_X_Forecast = Sixty_df_rev[Sixty_df_rev['tag'] == 'Forecast']\n",
    "\n",
    "# Forty df\n",
    "Forty_X_Train['tag'] = 'Train'\n",
    "Forty_X_Test['tag'] = 'Test'\n",
    "Forty_X_Forecast['tag'] = 'Forecast'\n",
    "\n",
    "Forty_df_rev = pd.concat([Forty_X_Train,Forty_X_Test,Forty_X_Forecast])\n",
    "\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "Forty_df_rev = pd.get_dummies(Forty_df_rev,columns=cols,drop_first=True)\n",
    "\n",
    "Forty_X_Train = Forty_df_rev[Forty_df_rev['tag'] == 'Train']\n",
    "Forty_X_Test = Forty_df_rev[Forty_df_rev['tag'] == 'Test']\n",
    "Forty_X_Forecast = Forty_df_rev[Forty_df_rev['tag'] == 'Forecast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict,TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'max_depth': 6, 'n_estimators': 100}\n",
    "\n",
    "# Grid Search with cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#Fit Random Forest Regressor with grid search best params\n",
    "rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],random_state=153, verbose=False)\n",
    "\n",
    "#### Non Zero df ####\n",
    "\n",
    "# Split train and test to Xtrain, Xtest, ytrain, ytest\n",
    "NonZero_X_Train = NonZero_X_Train.drop(columns=['batchID','tag'],axis = 1)\n",
    "NonZero_y_Train = NonZero_y_Train.drop(columns = ['batchID'],axis=1)\n",
    "\n",
    "# Cross Validation with Time Series validation\n",
    "NonZero_scores = cross_val_score(rfr, NonZero_X_Train,NonZero_y_Train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "### Forty Sixty df #####\n",
    "# Split train and test to Xtrain, Xtest, ytrain, ytest\n",
    "Forty_Sixty_df_X_Train = Forty_Sixty_df_X_Train.drop(columns=['batchID','tag'],axis = 1)\n",
    "Forty_Sixty_df_y_Train = Forty_Sixty_df_y_Train.drop(columns = ['batchID'],axis=1)\n",
    "\n",
    "# Cross Validation with Time Series validation\n",
    "Forty_Sixty_df_scores = cross_val_score(rfr, Forty_Sixty_df_X_Train,Forty_Sixty_df_y_Train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "### Forty df #####\n",
    "\n",
    "# Split train and test to Xtrain, Xtest, ytrain, ytest\n",
    "Forty_X_Train = Forty_X_Train.drop(columns=['batchID','tag'],axis = 1)\n",
    "Forty_y_Train = Forty_y_Train.drop(columns = ['batchID'],axis=1)\n",
    "\n",
    "# Cross Validation with Time Series validation\n",
    "Forty_scores = cross_val_score(rfr, Forty_X_Train,Forty_y_Train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "##### Sixty df #######\n",
    "\n",
    "# Split train and test to Xtrain, Xtest, ytrain, ytest\n",
    "Sixty_X_Train = Sixty_X_Train.drop(columns=['batchID','tag'],axis = 1)\n",
    "Sixty_y_Train = Sixty_y_Train.drop(columns = ['batchID'],axis=1)\n",
    "\n",
    "# Cross Validation with Time Series validation\n",
    "Sixty_scores = cross_val_score(rfr, Sixty_X_Train,Sixty_y_Train, cv=tscv, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.unique(NonZero_X_Test['batchID'])\n",
    "\n",
    "rf_mae = []\n",
    "batchID = []\n",
    "rf_result_dict = {}\n",
    "NonZero_rf_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    X_test_rf = NonZero_X_Test[NonZero_X_Test['batchID'] == i]\n",
    "    X_test_rf = X_test_rf.drop(columns = ['batchID','tag'],axis=1)\n",
    "    y_test_rf = test_results = NonZero_y_Test[NonZero_y_Test['batchID'] == i]\n",
    "    y_test_rf = y_test_rf.drop(columns=['batchID'],axis=1)\n",
    "    \n",
    "    predictions = cross_val_predict(rfr, X_test_rf, y_test_rf, cv=3)\n",
    "    \n",
    "    test_results['prediction'] = predictions\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    NonZero_rf_Forecasts = NonZero_rf_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(y_test_rf, predictions)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    rf_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "rf_result_dict = {'batchID':batchID,\n",
    "               'MAE':rf_mae}\n",
    "\n",
    "NonZero_randomForest = pd.DataFrame(rf_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.unique(Forty_Sixty_df_X_Test['batchID'])\n",
    "\n",
    "rf_mae = []\n",
    "batchID = []\n",
    "rf_result_dict = {}\n",
    "Forty_Sixty_df_rf_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    X_test_rf = Forty_Sixty_df_X_Test[Forty_Sixty_df_X_Test['batchID'] == i]\n",
    "    X_test_rf = X_test_rf.drop(columns = ['batchID','tag'],axis=1)\n",
    "    y_test_rf = test_results = Forty_Sixty_df_y_Test[Forty_Sixty_df_y_Test['batchID'] == i]\n",
    "    y_test_rf = y_test_rf.drop(columns=['batchID'],axis=1)\n",
    "    \n",
    "    predictions = cross_val_predict(rfr, X_test_rf, y_test_rf, cv=3)\n",
    "    \n",
    "    test_results['prediction'] = predictions\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Forty_Sixty_df_rf_Forecasts = Forty_Sixty_df_rf_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(y_test_rf, predictions)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    rf_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "rf_result_dict = {'batchID':batchID,\n",
    "               'MAE':rf_mae}\n",
    "\n",
    "Forty_Sixty_df_randomForest = pd.DataFrame(rf_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.unique(Forty_X_Test['batchID'])\n",
    "\n",
    "rf_mae = []\n",
    "batchID = []\n",
    "rf_result_dict = {}\n",
    "Forty_rf_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    X_test_rf = Forty_X_Test[Forty_X_Test['batchID'] == i]\n",
    "    X_test_rf = X_test_rf.drop(columns = ['batchID','tag'],axis=1)\n",
    "    y_test_rf = test_results = Forty_y_Test[Forty_y_Test['batchID'] == i]\n",
    "    y_test_rf = y_test_rf.drop(columns=['batchID'],axis=1)\n",
    "    \n",
    "    predictions = cross_val_predict(rfr, X_test_rf, y_test_rf, cv=3)\n",
    "    \n",
    "    test_results['prediction'] = predictions\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Forty_rf_Forecasts = Forty_rf_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(y_test_rf, predictions)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    rf_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "rf_result_dict = {'batchID':batchID,\n",
    "               'MAE':rf_mae}\n",
    "\n",
    "Forty_randomForest = pd.DataFrame(rf_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.unique(Sixty_X_Test['batchID'])\n",
    "\n",
    "rf_mae = []\n",
    "batchID = []\n",
    "rf_result_dict = {}\n",
    "Sixty_rf_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    X_test_rf = Sixty_X_Test[Sixty_X_Test['batchID'] == i]\n",
    "    X_test_rf = X_test_rf.drop(columns = ['batchID','tag'],axis=1)\n",
    "    y_test_rf = test_results = Sixty_y_Test[Sixty_y_Test['batchID'] == i]\n",
    "    y_test_rf = y_test_rf.drop(columns=['batchID'],axis=1)\n",
    "    \n",
    "    predictions = cross_val_predict(rfr, X_test_rf, y_test_rf, cv=3)\n",
    "    \n",
    "    test_results['prediction'] = predictions\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Sixty_rf_Forecasts = Sixty_rf_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(y_test_rf, predictions)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    rf_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "rf_result_dict = {'batchID':batchID,\n",
    "               'MAE':rf_mae}\n",
    "\n",
    "Sixty_randomForest = pd.DataFrame(rf_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgbr = xgb.XGBRegressor(random_state=153,max_depth=6,n_estimators=25)\n",
    "\n",
    "# Cross Validation with K fold validation\n",
    "NonZero_xgbr_scores = cross_val_score(xgbr, NonZero_X_Train,NonZero_y_Train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "Forty_Sixty_df_xgbr_scores = cross_val_score(xgbr, Forty_Sixty_df_X_Train,Forty_Sixty_df_y_Train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "Forty_xgbr_scores = cross_val_score(xgbr, Forty_X_Train,Forty_y_Train, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "Sixty_xgbr_scores = cross_val_score(xgbr, Sixty_X_Train,Sixty_y_Train, cv=tscv, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.unique(Forty_X_Test['batchID'])\n",
    "\n",
    "xgb_mae = []\n",
    "batchID = []\n",
    "xgboost_result_dict = {}\n",
    "Forty_xgboost_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    X_test_rf = Forty_X_Test[Forty_X_Test['batchID'] == i]\n",
    "    X_test_rf = X_test_rf.drop(columns = ['batchID','tag'],axis=1)\n",
    "    y_test_rf = test_results = Forty_y_Test[Forty_y_Test['batchID'] == i]\n",
    "    y_test_rf = y_test_rf.drop(columns=['batchID'],axis=1)\n",
    "    \n",
    "    predictions = cross_val_predict(xgbr, X_test_rf, y_test_rf, cv=3)\n",
    "    \n",
    "    test_results['prediction'] = predictions\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Forty_xgboost_Forecasts = Forty_xgboost_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(y_test_rf, predictions)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    xgb_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "xgboost_result_dict = {'batchID':batchID,\n",
    "               'MAE':xgb_mae}\n",
    "\n",
    "Forty_xgboost = pd.DataFrame(xgboost_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.unique(NonZero_X_Test['batchID'])\n",
    "\n",
    "xgb_mae = []\n",
    "batchID = []\n",
    "xgboost_result_dict = {}\n",
    "NonZero_xgboost_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    X_test_rf = NonZero_X_Test[NonZero_X_Test['batchID'] == i]\n",
    "    X_test_rf = X_test_rf.drop(columns = ['batchID','tag'],axis=1)\n",
    "    y_test_rf = test_results = NonZero_y_Test[NonZero_y_Test['batchID'] == i]\n",
    "    y_test_rf = y_test_rf.drop(columns=['batchID'],axis=1)\n",
    "    \n",
    "    predictions = cross_val_predict(xgbr, X_test_rf, y_test_rf, cv=3)\n",
    "    \n",
    "    test_results['prediction'] = predictions\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    NonZero_xgboost_Forecasts = NonZero_xgboost_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(y_test_rf, predictions)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    xgb_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "xgboost_result_dict = {'batchID':batchID,\n",
    "               'MAE':xgb_mae}\n",
    "\n",
    "NonZero_xgboost = pd.DataFrame(xgboost_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.unique(Forty_Sixty_df_X_Test['batchID'])\n",
    "\n",
    "xgb_mae = []\n",
    "batchID = []\n",
    "xgboost_result_dict = {}\n",
    "Forty_Sixty_df_xgboost_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    X_test_rf = Forty_Sixty_df_X_Test[Forty_Sixty_df_X_Test['batchID'] == i]\n",
    "    X_test_rf = X_test_rf.drop(columns = ['batchID','tag'],axis=1)\n",
    "    y_test_rf = test_results = Forty_Sixty_df_y_Test[Forty_Sixty_df_y_Test['batchID'] == i]\n",
    "    y_test_rf = y_test_rf.drop(columns=['batchID'],axis=1)\n",
    "    \n",
    "    predictions = cross_val_predict(xgbr, X_test_rf, y_test_rf, cv=3)\n",
    "    \n",
    "    test_results['prediction'] = predictions\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Forty_Sixty_df_xgboost_Forecasts = Forty_Sixty_df_xgboost_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(y_test_rf, predictions)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    xgb_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "xgboost_result_dict = {'batchID':batchID,\n",
    "               'MAE':xgb_mae}\n",
    "\n",
    "Forty_Sixty_df_xgboost = pd.DataFrame(xgboost_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = np.unique(Sixty_X_Test['batchID'])\n",
    "\n",
    "xgb_mae = []\n",
    "batchID = []\n",
    "xgboost_result_dict = {}\n",
    "Sixty_xgboost_Forecasts = pd.DataFrame()\n",
    "\n",
    "for i in batch_ids:\n",
    "    X_test_rf = Sixty_X_Test[Sixty_X_Test['batchID'] == i]\n",
    "    X_test_rf = X_test_rf.drop(columns = ['batchID','tag'],axis=1)\n",
    "    y_test_rf = test_results = Sixty_y_Test[Sixty_y_Test['batchID'] == i]\n",
    "    y_test_rf = y_test_rf.drop(columns=['batchID'],axis=1)\n",
    "    \n",
    "    predictions = cross_val_predict(xgbr, X_test_rf, y_test_rf, cv=3)\n",
    "    \n",
    "    test_results['prediction'] = predictions\n",
    "    test_results['residual'] = test_results['value'] - test_results['prediction']\n",
    "    test_results = test_results.rename(columns = {'value':'actual'})\n",
    "    Sixty_xgboost_Forecasts = Sixty_xgboost_Forecasts.append(test_results)\n",
    "    \n",
    "    #Error Metrics - Mean Absolute Error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    MAE = mean_absolute_error(y_test_rf, predictions)\n",
    "    \n",
    "    #Append MAE and batchID to list\n",
    "    xgb_mae.append(MAE)\n",
    "    batchID.append(i)\n",
    "\n",
    "xgboost_result_dict = {'batchID':batchID,\n",
    "               'MAE':xgb_mae}\n",
    "\n",
    "Sixty_xgboost = pd.DataFrame(xgboost_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4180976939685976\n"
     ]
    }
   ],
   "source": [
    "# Non Zero df - MAE Comparisons\n",
    "NonZero_xgboost.columns = ['batchID','xgb_MAE']\n",
    "NonZero_randomForest.columns = ['batchID','rf_MAE']\n",
    "NonZero_lightgbm_MAE.columns = ['batchID','lgbm_MAE']\n",
    "NonZero_catboost_MAE.columns = ['batchID','cb_MAE']\n",
    "NonZero_Croston_MAE = NonZero_Croston_MAE.reset_index()\n",
    "\n",
    "# Non Zero results\n",
    "NonZero_results = pd.merge(NonZero_df_Naive_results,NonZero_xgboost,how='inner',on='batchID')\n",
    "NonZero_results = pd.merge(NonZero_results,NonZero_Croston_MAE,how='inner',on='batchID')\n",
    "NonZero_results = pd.merge(NonZero_results,NonZero_catboost_MAE,how='inner',on='batchID')\n",
    "NonZero_results = pd.merge(NonZero_results,NonZero_randomForest,how='inner',on='batchID')\n",
    "NonZero_results = pd.merge(NonZero_results,NonZero_lightgbm_MAE,how='inner',on='batchID')\n",
    "NonZero_results = pd.merge(NonZero_results,insample_naive,how='inner',on='batchID')\n",
    "\n",
    "# Find minimum MAE for each batch ID \n",
    "NonZero_results['Min'] = NonZero_results[['rf_MAE','xgb_MAE','lgbm_MAE','cb_MAE','Naive_MAE','Croston_MAE']].min(axis=1)\n",
    "\n",
    "# Find which model has minimum MAE\n",
    "NonZero_results['MinMAE_Model'] = NonZero_results[['rf_MAE','xgb_MAE','lgbm_MAE','cb_MAE','Naive_MAE','Croston_MAE']].idxmin(axis=1)\n",
    "NonZero_results['MASE'] = NonZero_results['Min']/NonZero_results['Insample-MAE']\n",
    "print(NonZero_results['MASE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.004103741131517\n"
     ]
    }
   ],
   "source": [
    "# Forty df - MAE Comparisons\n",
    "Forty_Sixty_df_xgboost.columns = ['batchID','Forty_Sixty_df_xgb_MAE']\n",
    "Forty_Sixty_df_randomForest.columns = ['batchID','Forty_Sixty_df_rf_MAE']\n",
    "Forty_Sixty_df_lightgbm_MAE.columns = ['batchID','Forty_Sixty_df_lgbm_MAE']\n",
    "Forty_Sixty_df_catboost_MAE.columns = ['batchID','Forty_Sixty_df_cb_MAE']\n",
    "Forty_Sixty_df_Croston_MAE = Forty_Sixty_df_Croston_MAE.reset_index()\n",
    "Forty_Sixty_df_Croston_MAE.columns = ['batchID','Forty_Sixty_df_Croston_MAE']\n",
    "Forty_Sixty_df_Naive_results.columns = ['batchID','Forty_Sixty_df_Naive_MAE']\n",
    "\n",
    "Sixty_xgboost.columns = ['batchID','Sixty_xgb_MAE']\n",
    "Sixty_randomForest.columns = ['batchID','Sixty_rf_MAE']\n",
    "Sixty_lightgbm_MAE.columns = ['batchID','Sixty_lgbm_MAE']\n",
    "Sixty_catboost_MAE.columns = ['batchID','Sixty_cb_MAE']\n",
    "Sixty_Croston_MAE = Sixty_Croston_MAE.reset_index()\n",
    "Sixty_Croston_MAE.columns = ['batchID','Sixty_Croston_MAE']\n",
    "Sixty_df_Naive_results.columns = ['batchID','Sixty_df_Naive_MAE']\n",
    "\n",
    "Forty_xgboost.columns = ['batchID','Forty_xgb_MAE']\n",
    "Forty_randomForest.columns = ['batchID','Forty_rf_MAE']\n",
    "Forty_lightgbm_MAE.columns = ['batchID','Forty_lgbm_MAE']\n",
    "Forty_catboost_MAE.columns = ['batchID','Forty_cb_MAE']\n",
    "Forty_Croston_MAE = Forty_Croston_MAE.reset_index()\n",
    "Forty_Croston_MAE.columns = ['batchID','Forty_Croston_MAE']\n",
    "Forty_df_Naive_results.columns = ['batchID','Forty_df_Naive_MAE']\n",
    "\n",
    "# Forty results\n",
    "results = pd.merge(Forty_Sixty_df_Naive_results,Forty_Sixty_df_xgboost,how='inner',on='batchID')\n",
    "results = pd.merge(results,Forty_Sixty_df_Croston_MAE,how='inner',on='batchID')\n",
    "results = pd.merge(results,Forty_Sixty_df_catboost_MAE,how='inner',on='batchID')\n",
    "results = pd.merge(results,Forty_Sixty_df_randomForest,how='inner',on='batchID')\n",
    "results = pd.merge(results,Forty_Sixty_df_lightgbm_MAE,how='inner',on='batchID')\n",
    "\n",
    "\n",
    "results = pd.merge(results,Sixty_df_Naive_results,how='left',on='batchID')\n",
    "results = pd.merge(results,Sixty_xgboost,how='left',on='batchID')\n",
    "results = pd.merge(results,Sixty_Croston_MAE,how='left',on='batchID')\n",
    "results = pd.merge(results,Sixty_catboost_MAE,how='left',on='batchID')\n",
    "results = pd.merge(results,Sixty_randomForest,how='left',on='batchID')\n",
    "results = pd.merge(results,Sixty_lightgbm_MAE,how='left',on='batchID')\n",
    "\n",
    "results = pd.merge(results,Forty_df_Naive_results,how='left',on='batchID')\n",
    "results = pd.merge(results,Forty_xgboost,how='left',on='batchID')\n",
    "results = pd.merge(results,Forty_Croston_MAE,how='left',on='batchID')\n",
    "results = pd.merge(results,Forty_catboost_MAE,how='left',on='batchID')\n",
    "results = pd.merge(results,Forty_randomForest,how='left',on='batchID')\n",
    "results = pd.merge(results,Forty_lightgbm_MAE,how='left',on='batchID')\n",
    "results = pd.merge(results,insample_naive,how='left',on='batchID')\n",
    "\n",
    "# Find minimum MAE for each batch ID \n",
    "results['Min'] = results[['Forty_Sixty_df_xgb_MAE','Forty_Sixty_df_rf_MAE','Forty_Sixty_df_lgbm_MAE',\n",
    "      'Forty_Sixty_df_cb_MAE','Forty_Sixty_df_Croston_MAE','Sixty_xgb_MAE','Sixty_rf_MAE',\n",
    "      'Sixty_lgbm_MAE','Sixty_cb_MAE','Sixty_Croston_MAE','Forty_xgb_MAE','Forty_rf_MAE',\n",
    "      'Forty_lgbm_MAE','Forty_cb_MAE','Forty_Croston_MAE','Forty_df_Naive_MAE','Sixty_df_Naive_MAE','Forty_Sixty_df_Naive_MAE']].min(axis=1)\n",
    "\n",
    "# Find which model has minimum MAE\n",
    "results['MinMAE_Model'] = results[['Forty_Sixty_df_xgb_MAE','Forty_Sixty_df_rf_MAE','Forty_Sixty_df_lgbm_MAE',\n",
    "      'Forty_Sixty_df_cb_MAE','Forty_Sixty_df_Croston_MAE','Sixty_xgb_MAE','Sixty_rf_MAE',\n",
    "      'Sixty_lgbm_MAE','Sixty_cb_MAE','Sixty_Croston_MAE','Forty_xgb_MAE','Forty_rf_MAE',\n",
    "      'Forty_lgbm_MAE','Forty_cb_MAE','Forty_Croston_MAE','Forty_df_Naive_MAE','Sixty_df_Naive_MAE','Forty_Sixty_df_Naive_MAE']].idxmin(axis=1)\n",
    "results['MASE'] = results['Min']/results['Insample-MAE']\n",
    "print(results['MASE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Best model\n",
    "best_model = pd.concat([results[['batchID','MASE','MinMAE_Model']].sort_values(by='MASE',ascending=False),NonZero_results[['batchID','MASE','MinMAE_Model']].sort_values(by='MASE',ascending=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Forty_Sixty_df_rf_MAE         87\n",
       "Naive_MAE                     81\n",
       "Croston_MAE                   75\n",
       "rf_MAE                        74\n",
       "lgbm_MAE                      74\n",
       "xgb_MAE                       61\n",
       "Forty_Sixty_df_Croston_MAE    54\n",
       "cb_MAE                        52\n",
       "Forty_Sixty_df_xgb_MAE        49\n",
       "Forty_df_Naive_MAE            40\n",
       "Forty_Sixty_df_lgbm_MAE       40\n",
       "Sixty_df_Naive_MAE            25\n",
       "Forty_lgbm_MAE                25\n",
       "Forty_Sixty_df_cb_MAE         24\n",
       "Sixty_lgbm_MAE                15\n",
       "Sixty_cb_MAE                  13\n",
       "Forty_cb_MAE                  11\n",
       "Forty_rf_MAE                   3\n",
       "Sixty_rf_MAE                   2\n",
       "Name: MinMAE_Model, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the best model to forecast \n",
    "best_model['MinMAE_Model'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
